# lib/brain/graph/nodes/llm_inference.py
"""
ãƒãƒ¼ãƒ‰: LLM Brain æ¨è«–

Claude API + Function Calling ã§ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‡¦ç†ã€‚
æ„å›³ç†è§£ãƒ»Toolé¸æŠãƒ»ãƒ†ã‚­ã‚¹ãƒˆå¿œç­”ç”Ÿæˆã‚’è¡Œã†ã€‚
"""

import logging
import time
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from lib.brain.core import SoulkunBrain

from lib.brain.graph.state import BrainGraphState

logger = logging.getLogger(__name__)


def make_llm_inference(brain: "SoulkunBrain"):
    """SoulkunBrainã‚’å‚ç…§ã™ã‚‹llm_inferenceãƒãƒ¼ãƒ‰ã‚’ç”Ÿæˆ"""

    async def llm_inference(state: BrainGraphState) -> dict:
        from lib.brain.core import _validate_llm_result_type, _extract_confidence_value

        start_time = state["start_time"]

        try:
            t0 = time.time()
            llm_result = await brain.llm_brain.process(
                context=state["llm_context"],
                message=state["message"],
                tools=state["tools"],
            )
            logger.debug(
                "[graph:llm_inference] DONE t=%.3fs (took %.3fs)",
                time.time() - start_time, time.time() - t0,
            )

            # å¢ƒç•Œå‹æ¤œè¨¼
            _validate_llm_result_type(llm_result, "graph:llm_inference")

            confidence_value = _extract_confidence_value(
                llm_result.confidence, "graph:llm_inference:confidence"
            )

            # ãƒ„ãƒ¼ãƒ«åã®ã¿è¨˜éŒ²ï¼ˆå¼•æ•°ã¯PIIã‚’å«ã‚€å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚é™¤å¤–ï¼‰
            _tool_names = [tc.tool_name for tc in (llm_result.tool_calls or [])] if llm_result.tool_calls else []
            logger.info(
                "ğŸ§  [DIAG] LLM_BRAIN PATH: tool_calls=%d, tools=%s, has_text=%s, confidence=%.2f",
                len(llm_result.tool_calls or []),
                _tool_names,
                llm_result.text_response is not None,
                confidence_value,
            )

            return {
                "llm_result": llm_result,
                "confidence_value": confidence_value,
            }
        except Exception as e:
            logger.error("[graph:llm_inference] Error: %s", e, exc_info=True)
            raise

    return llm_inference
