# lib/brain/llm_observability.py
"""
LLM Brain å°‚ç”¨ Observability Layer

è¨­è¨ˆæ›¸: docs/25_llm_native_brain_architecture.md ã‚»ã‚¯ã‚·ãƒ§ãƒ³15

ã€ç›®çš„ã€‘
LLM Brain ã®å…¨åˆ¤æ–­éç¨‹ã‚’è¨˜éŒ²ã—ã€ä»¥ä¸‹ã‚’å®Ÿç¾ã™ã‚‹:
- æœ¬ç•ªãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ï¼ˆå¿œç­”æ™‚é–“ã€ã‚¨ãƒ©ãƒ¼ç‡ã€ã‚³ã‚¹ãƒˆï¼‰
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åé›†
- éšœå®³æ™‚ã®ãƒ‡ãƒãƒƒã‚°

ã€è¨˜éŒ²ã™ã‚‹æƒ…å ±ã€‘
- å…¥åŠ›ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆå…ˆé ­200æ–‡å­—ã€ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ä¿è­·ï¼‰
- LLMã®æ€è€ƒéç¨‹ï¼ˆChain-of-Thoughtï¼‰
- é¸æŠã•ã‚ŒãŸToolãƒ»ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- Guardian/AuthGateã®åˆ¤å®šçµæœ
- å®Ÿè¡Œçµæœ
- ã‚³ã‚¹ãƒˆæƒ…å ±

ã€10ã®é‰„å‰‡ã€‘
- #1 å…¨ãƒ†ãƒ¼ãƒ–ãƒ«ã«organization_id â†’ å…¨ãƒ­ã‚°ã«org_idã‚’ä»˜ä¸
- #3 ç›£æŸ»ãƒ­ã‚°è¨˜éŒ² â†’ confidentialã¨ã—ã¦è¨˜éŒ²
- #9 SQLãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ– â†’ ãƒ—ãƒªãƒšã‚¢ãƒ‰ã‚¹ãƒ†ãƒ¼ãƒˆãƒ¡ãƒ³ãƒˆä½¿ç”¨

Author: Claude Opus 4.5
Created: 2026-01-31
"""

from __future__ import annotations

import os
import hashlib
import logging
from dataclasses import dataclass, field
from datetime import datetime
from decimal import Decimal
from typing import Any, Dict, List, Optional, TYPE_CHECKING
from enum import Enum

if TYPE_CHECKING:
    from lib.brain.llm_brain import LLMBrainResult, ToolCall, ConfidenceScores
    from lib.brain.guardian_layer import GuardianResult

logger = logging.getLogger(__name__)


# =============================================================================
# å®šæ•°
# =============================================================================

# ã‚³ã‚¹ãƒˆè¨ˆç®—ç”¨ï¼ˆGPT-5.2 via OpenRouterã€2026-01æ™‚ç‚¹ï¼‰
# å‚ç…§: https://openrouter.ai/openai/gpt-5.2
COST_PER_1M_INPUT_TOKENS_USD = 1.75   # $1.75/M input tokens
COST_PER_1M_OUTPUT_TOKENS_USD = 14.0  # $14.00/M output tokens
USD_TO_JPY = 154  # ç‚ºæ›¿ãƒ¬ãƒ¼ãƒˆï¼ˆæ¦‚ç®—ï¼‰

# ç’°å¢ƒ
ENVIRONMENT = os.getenv("ENVIRONMENT", "production")
VERSION = os.getenv("SOULKUN_VERSION", "10.53.5")


# =============================================================================
# Enum
# =============================================================================

class LLMLogType(str, Enum):
    """LLM Brain ãƒ­ã‚°ã‚¿ã‚¤ãƒ—"""
    LLM_PROCESS = "llm_process"           # LLMå‡¦ç†å…¨ä½“
    GUARDIAN_CHECK = "guardian_check"     # Guardianåˆ¤å®š
    AUTH_GATE_CHECK = "auth_gate_check"   # Authorization Gateåˆ¤å®š
    TOOL_EXECUTION = "tool_execution"     # Toolå®Ÿè¡Œ
    CONFIRMATION = "confirmation"          # ç¢ºèªãƒ•ãƒ­ãƒ¼
    ERROR = "error"                        # ã‚¨ãƒ©ãƒ¼


class ConfirmationStatus(str, Enum):
    """ç¢ºèªãƒ•ãƒ­ãƒ¼ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹"""
    PENDING = "pending"
    APPROVED = "approved"
    DENIED = "denied"
    TIMEOUT = "timeout"


# =============================================================================
# ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹
# =============================================================================

@dataclass
class LLMBrainLog:
    """
    LLM Brain ãƒ­ã‚°ã‚¨ãƒ³ãƒˆãƒª

    brain_observability_logs ãƒ†ãƒ¼ãƒ–ãƒ«ã«å¯¾å¿œã™ã‚‹ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã€‚
    """
    # ========================================
    # è­˜åˆ¥æƒ…å ±
    # ========================================
    organization_id: str
    user_id: str
    room_id: Optional[str] = None
    session_id: Optional[str] = None

    # ========================================
    # ãƒªã‚¯ã‚¨ã‚¹ãƒˆæƒ…å ±
    # ========================================
    message_preview: Optional[str] = None
    message_hash: Optional[str] = None

    # ========================================
    # LLM Brain åˆ¤æ–­æƒ…å ±
    # ========================================
    detected_intent: Optional[str] = None
    output_type: str = "text_response"
    reasoning: Optional[str] = None

    # Toolæƒ…å ±
    tool_name: Optional[str] = None
    tool_parameters: Optional[Dict[str, Any]] = None
    tool_count: int = 0

    # ç¢ºä¿¡åº¦
    confidence_overall: Optional[float] = None
    confidence_intent: Optional[float] = None
    confidence_parameters: Optional[float] = None

    # ========================================
    # Guardian Layer åˆ¤å®šæƒ…å ±
    # ========================================
    guardian_action: Optional[str] = None
    guardian_reason: Optional[str] = None
    guardian_risk_level: Optional[str] = None
    guardian_check_type: Optional[str] = None

    # ========================================
    # Authorization Gate åˆ¤å®šæƒ…å ±
    # ========================================
    auth_gate_action: Optional[str] = None
    auth_gate_reason: Optional[str] = None

    # ========================================
    # å®Ÿè¡Œçµæœ
    # ========================================
    execution_success: Optional[bool] = None
    execution_error_code: Optional[str] = None
    execution_error_message: Optional[str] = None
    execution_result_summary: Optional[str] = None

    # ========================================
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ»ã‚³ã‚¹ãƒˆæƒ…å ±
    # ========================================
    model_used: Optional[str] = None
    api_provider: Optional[str] = None
    input_tokens: int = 0
    output_tokens: int = 0
    llm_response_time_ms: Optional[int] = None
    guardian_check_time_ms: Optional[int] = None
    total_response_time_ms: Optional[int] = None
    estimated_cost_yen: Optional[Decimal] = None

    # ========================================
    # ç¢ºèªãƒ•ãƒ­ãƒ¼æƒ…å ±
    # ========================================
    needs_confirmation: bool = False
    confirmation_question: Optional[str] = None
    confirmation_status: Optional[str] = None

    # ========================================
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    # ========================================
    environment: str = field(default_factory=lambda: ENVIRONMENT)
    version: str = field(default_factory=lambda: VERSION)
    created_at: datetime = field(default_factory=datetime.utcnow)

    def calculate_cost(self) -> Decimal:
        """
        ã‚³ã‚¹ãƒˆã‚’è¨ˆç®—ï¼ˆå††ï¼‰

        GPT-5.2ã®æ–™é‡‘:
        - å…¥åŠ›: $1.75/1M tokens
        - å‡ºåŠ›: $14.00/1M tokens
        """
        input_cost_usd = (self.input_tokens / 1_000_000) * COST_PER_1M_INPUT_TOKENS_USD
        output_cost_usd = (self.output_tokens / 1_000_000) * COST_PER_1M_OUTPUT_TOKENS_USD
        total_usd = input_cost_usd + output_cost_usd
        total_yen = Decimal(str(total_usd)) * Decimal(str(USD_TO_JPY))
        return round(total_yen, 4)

    def to_dict(self) -> Dict[str, Any]:
        """DBæŒ¿å…¥ç”¨ã®è¾æ›¸ã‚’ç”Ÿæˆ"""
        # ã‚³ã‚¹ãƒˆã‚’è¨ˆç®—
        if self.estimated_cost_yen is None:
            self.estimated_cost_yen = self.calculate_cost()

        return {
            "organization_id": self.organization_id,
            "user_id": self.user_id,
            "room_id": self.room_id,
            "session_id": self.session_id,
            "message_preview": self.message_preview,
            "message_hash": self.message_hash,
            "detected_intent": self.detected_intent,
            "output_type": self.output_type,
            "reasoning": self.reasoning,
            "tool_name": self.tool_name,
            "tool_parameters": self.tool_parameters,
            "tool_count": self.tool_count,
            "confidence_overall": self.confidence_overall,
            "confidence_intent": self.confidence_intent,
            "confidence_parameters": self.confidence_parameters,
            "guardian_action": self.guardian_action,
            "guardian_reason": self.guardian_reason,
            "guardian_risk_level": self.guardian_risk_level,
            "guardian_check_type": self.guardian_check_type,
            "auth_gate_action": self.auth_gate_action,
            "auth_gate_reason": self.auth_gate_reason,
            "execution_success": self.execution_success,
            "execution_error_code": self.execution_error_code,
            "execution_error_message": self.execution_error_message,
            "execution_result_summary": self.execution_result_summary,
            "model_used": self.model_used,
            "api_provider": self.api_provider,
            "input_tokens": self.input_tokens,
            "output_tokens": self.output_tokens,
            "llm_response_time_ms": self.llm_response_time_ms,
            "guardian_check_time_ms": self.guardian_check_time_ms,
            "total_response_time_ms": self.total_response_time_ms,
            "estimated_cost_yen": float(self.estimated_cost_yen) if self.estimated_cost_yen else None,
            "needs_confirmation": self.needs_confirmation,
            "confirmation_question": self.confirmation_question,
            "confirmation_status": self.confirmation_status,
            "environment": self.environment,
            "version": self.version,
            "created_at": self.created_at.isoformat(),
        }

    def to_log_string(self) -> str:
        """Cloud Loggingç”¨ã®æ§‹é€ åŒ–ãƒ­ã‚°æ–‡å­—åˆ—"""
        parts = [
            f"ğŸ§  LLM Brain Log",
            f"user={self.user_id}",
            f"type={self.output_type}",
        ]

        if self.tool_name:
            parts.append(f"tool={self.tool_name}")

        if self.confidence_overall is not None:
            parts.append(f"conf={self.confidence_overall:.2f}")

        if self.guardian_action:
            parts.append(f"guardian={self.guardian_action}")

        if self.execution_success is not None:
            status = "ok" if self.execution_success else "error"
            parts.append(f"exec={status}")

        if self.total_response_time_ms:
            parts.append(f"time={self.total_response_time_ms}ms")

        if self.estimated_cost_yen:
            parts.append(f"cost=Â¥{self.estimated_cost_yen:.2f}")

        return " ".join(parts)


# =============================================================================
# LLMBrainObservability ã‚¯ãƒ©ã‚¹
# =============================================================================

class LLMBrainObservability:
    """
    LLM Brain å°‚ç”¨ Observability

    Usage:
        obs = LLMBrainObservability(pool=db_pool, org_id="org_xxx")

        # LLMå‡¦ç†çµæœã‚’ãƒ­ã‚°
        log = obs.log_llm_process(
            user_id="12345",
            message="ã‚¿ã‚¹ã‚¯è¿½åŠ ã—ã¦",
            result=llm_brain_result,
            response_time_ms=1500,
        )

        # Guardianåˆ¤å®šã‚’ãƒ­ã‚°
        obs.log_guardian_check(
            log_entry=log,
            guardian_result=guardian_result,
            check_time_ms=50,
        )

        # DBã«ä¿å­˜
        await obs.save(log)
    """

    def __init__(
        self,
        pool=None,
        org_id: str = "",
        enable_cloud_logging: bool = True,
        enable_persistence: bool = True,
    ):
        """
        åˆæœŸåŒ–

        Args:
            pool: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šãƒ—ãƒ¼ãƒ«ï¼ˆasyncpgï¼‰
            org_id: çµ„ç¹”ID
            enable_cloud_logging: Cloud Loggingã¸ã®å‡ºåŠ›ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã‹
            enable_persistence: DBã¸ã®æ°¸ç¶šåŒ–ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã‹
        """
        self.pool = pool
        self.org_id = org_id
        self.enable_cloud_logging = enable_cloud_logging
        self.enable_persistence = enable_persistence

        # ãƒãƒƒãƒ•ã‚¡ï¼ˆãƒãƒƒãƒæŒ¿å…¥ç”¨ï¼‰
        self._buffer: List[LLMBrainLog] = []
        self._buffer_max_size = 100

        logger.debug(
            f"LLMBrainObservability initialized: "
            f"org_id={org_id}, persistence={enable_persistence}"
        )

    # =========================================================================
    # ãƒ­ã‚°ä½œæˆãƒ¡ã‚½ãƒƒãƒ‰
    # =========================================================================

    def create_log(
        self,
        user_id: str,
        room_id: Optional[str] = None,
        session_id: Optional[str] = None,
        message: Optional[str] = None,
    ) -> LLMBrainLog:
        """
        æ–°ã—ã„ãƒ­ã‚°ã‚¨ãƒ³ãƒˆãƒªã‚’ä½œæˆ

        Args:
            user_id: ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
            room_id: ãƒ«ãƒ¼ãƒ ID
            session_id: ã‚»ãƒƒã‚·ãƒ§ãƒ³ID
            message: å…¥åŠ›ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸

        Returns:
            LLMBrainLog
        """
        log = LLMBrainLog(
            organization_id=self.org_id,
            user_id=user_id,
            room_id=room_id,
            session_id=session_id,
        )

        if message:
            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆå…ˆé ­200æ–‡å­—ã€ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ä¿è­·ï¼‰
            log.message_preview = message[:200].replace('\n', ' ')
            # ãƒãƒƒã‚·ãƒ¥ï¼ˆé‡è¤‡æ¤œå‡ºç”¨ï¼‰
            log.message_hash = hashlib.sha256(message.encode()).hexdigest()

        return log

    def log_llm_process(
        self,
        user_id: str,
        message: str,
        result: "LLMBrainResult",
        response_time_ms: int,
        room_id: Optional[str] = None,
        session_id: Optional[str] = None,
    ) -> LLMBrainLog:
        """
        LLMå‡¦ç†çµæœã‚’ãƒ­ã‚°

        Args:
            user_id: ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
            message: å…¥åŠ›ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            result: LLMBrainResult
            response_time_ms: LLMå¿œç­”æ™‚é–“ï¼ˆãƒŸãƒªç§’ï¼‰
            room_id: ãƒ«ãƒ¼ãƒ ID
            session_id: ã‚»ãƒƒã‚·ãƒ§ãƒ³ID

        Returns:
            LLMBrainLog
        """
        log = self.create_log(user_id, room_id, session_id, message)

        # LLM Brainçµæœã‚’è¨­å®š
        log.output_type = result.output_type
        log.reasoning = result.reasoning
        log.model_used = result.model_used
        log.api_provider = result.api_provider
        log.input_tokens = result.input_tokens
        log.output_tokens = result.output_tokens
        log.llm_response_time_ms = response_time_ms

        # ç¢ºä¿¡åº¦
        if result.confidence:
            log.confidence_overall = result.confidence.overall
            log.confidence_intent = result.confidence.intent
            log.confidence_parameters = result.confidence.parameters

        # Toolæƒ…å ±
        if result.tool_calls:
            log.tool_count = len(result.tool_calls)
            if result.tool_calls:
                first_tool = result.tool_calls[0]
                log.tool_name = first_tool.tool_name
                log.tool_parameters = first_tool.parameters

        # ç¢ºèªæƒ…å ±
        log.needs_confirmation = result.needs_confirmation
        log.confirmation_question = result.confirmation_question
        if result.needs_confirmation:
            log.confirmation_status = ConfirmationStatus.PENDING.value

        # ã‚³ã‚¹ãƒˆè¨ˆç®—
        log.estimated_cost_yen = log.calculate_cost()

        # Cloud Loggingã«å‡ºåŠ›
        if self.enable_cloud_logging:
            logger.info(log.to_log_string())

        return log

    def log_guardian_check(
        self,
        log_entry: LLMBrainLog,
        guardian_result: "GuardianResult",
        check_time_ms: int,
    ) -> LLMBrainLog:
        """
        Guardian Layeråˆ¤å®šçµæœã‚’ãƒ­ã‚°ã«è¿½åŠ 

        Args:
            log_entry: æ—¢å­˜ã®ãƒ­ã‚°ã‚¨ãƒ³ãƒˆãƒª
            guardian_result: GuardianResult
            check_time_ms: Guardianåˆ¤å®šæ™‚é–“ï¼ˆãƒŸãƒªç§’ï¼‰

        Returns:
            æ›´æ–°ã•ã‚ŒãŸLLMBrainLog
        """
        log_entry.guardian_action = guardian_result.action.value
        log_entry.guardian_reason = guardian_result.reason
        log_entry.guardian_check_time_ms = check_time_ms

        if hasattr(guardian_result, 'risk_level') and guardian_result.risk_level:
            log_entry.guardian_risk_level = guardian_result.risk_level.value

        if hasattr(guardian_result, 'check_type') and guardian_result.check_type:
            log_entry.guardian_check_type = guardian_result.check_type

        # ç·å¿œç­”æ™‚é–“ã‚’æ›´æ–°
        if log_entry.llm_response_time_ms:
            log_entry.total_response_time_ms = (
                log_entry.llm_response_time_ms + check_time_ms
            )

        return log_entry

    def log_execution_result(
        self,
        log_entry: LLMBrainLog,
        success: bool,
        result_summary: Optional[str] = None,
        error_code: Optional[str] = None,
        error_message: Optional[str] = None,
        execution_time_ms: Optional[int] = None,
    ) -> LLMBrainLog:
        """
        å®Ÿè¡Œçµæœã‚’ãƒ­ã‚°ã«è¿½åŠ 

        Args:
            log_entry: æ—¢å­˜ã®ãƒ­ã‚°ã‚¨ãƒ³ãƒˆãƒª
            success: æˆåŠŸã—ãŸã‹
            result_summary: çµæœæ¦‚è¦
            error_code: ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰
            error_message: ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            execution_time_ms: å®Ÿè¡Œæ™‚é–“ï¼ˆãƒŸãƒªç§’ï¼‰

        Returns:
            æ›´æ–°ã•ã‚ŒãŸLLMBrainLog
        """
        log_entry.execution_success = success
        log_entry.execution_result_summary = result_summary[:500] if result_summary else None
        log_entry.execution_error_code = error_code
        log_entry.execution_error_message = error_message[:500] if error_message else None

        # ç·å¿œç­”æ™‚é–“ã‚’æ›´æ–°
        if execution_time_ms and log_entry.total_response_time_ms:
            log_entry.total_response_time_ms += execution_time_ms

        return log_entry

    def log_confirmation_response(
        self,
        log_entry: LLMBrainLog,
        status: ConfirmationStatus,
    ) -> LLMBrainLog:
        """
        ç¢ºèªå¿œç­”ã‚’ãƒ­ã‚°ã«è¿½åŠ 

        Args:
            log_entry: æ—¢å­˜ã®ãƒ­ã‚°ã‚¨ãƒ³ãƒˆãƒª
            status: ç¢ºèªã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹

        Returns:
            æ›´æ–°ã•ã‚ŒãŸLLMBrainLog
        """
        log_entry.confirmation_status = status.value
        return log_entry

    # =========================================================================
    # æ°¸ç¶šåŒ–ãƒ¡ã‚½ãƒƒãƒ‰
    # =========================================================================

    async def save(self, log_entry: LLMBrainLog) -> Optional[str]:
        """
        ãƒ­ã‚°ã‚’DBã«ä¿å­˜

        Args:
            log_entry: ãƒ­ã‚°ã‚¨ãƒ³ãƒˆãƒª

        Returns:
            ä¿å­˜ã•ã‚ŒãŸãƒ­ã‚°ã®IDã€ã¾ãŸã¯None
        """
        if not self.enable_persistence or not self.pool:
            logger.debug("Persistence disabled or no pool, skipping save")
            return None

        try:
            data = log_entry.to_dict()

            async with self.pool.acquire() as conn:
                result = await conn.fetchrow(
                    """
                    INSERT INTO brain_observability_logs (
                        organization_id, user_id, room_id, session_id,
                        message_preview, message_hash,
                        detected_intent, output_type, reasoning,
                        tool_name, tool_parameters, tool_count,
                        confidence_overall, confidence_intent, confidence_parameters,
                        guardian_action, guardian_reason, guardian_risk_level, guardian_check_type,
                        auth_gate_action, auth_gate_reason,
                        execution_success, execution_error_code, execution_error_message, execution_result_summary,
                        model_used, api_provider, input_tokens, output_tokens,
                        llm_response_time_ms, guardian_check_time_ms, total_response_time_ms,
                        estimated_cost_yen,
                        needs_confirmation, confirmation_question, confirmation_status,
                        environment, version
                    ) VALUES (
                        $1, $2, $3, $4, $5, $6, $7, $8, $9, $10,
                        $11, $12, $13, $14, $15, $16, $17, $18, $19, $20,
                        $21, $22, $23, $24, $25, $26, $27, $28, $29, $30,
                        $31, $32, $33, $34, $35, $36, $37, $38
                    )
                    RETURNING id
                    """,
                    data["organization_id"],
                    data["user_id"],
                    data["room_id"],
                    data["session_id"],
                    data["message_preview"],
                    data["message_hash"],
                    data["detected_intent"],
                    data["output_type"],
                    data["reasoning"],
                    data["tool_name"],
                    data["tool_parameters"],
                    data["tool_count"],
                    data["confidence_overall"],
                    data["confidence_intent"],
                    data["confidence_parameters"],
                    data["guardian_action"],
                    data["guardian_reason"],
                    data["guardian_risk_level"],
                    data["guardian_check_type"],
                    data["auth_gate_action"],
                    data["auth_gate_reason"],
                    data["execution_success"],
                    data["execution_error_code"],
                    data["execution_error_message"],
                    data["execution_result_summary"],
                    data["model_used"],
                    data["api_provider"],
                    data["input_tokens"],
                    data["output_tokens"],
                    data["llm_response_time_ms"],
                    data["guardian_check_time_ms"],
                    data["total_response_time_ms"],
                    data["estimated_cost_yen"],
                    data["needs_confirmation"],
                    data["confirmation_question"],
                    data["confirmation_status"],
                    data["environment"],
                    data["version"],
                )

                log_id = str(result["id"]) if result else None
                logger.debug(f"Saved LLM Brain log: {log_id}")
                return log_id

        except Exception as e:
            logger.error(f"Failed to save LLM Brain log: {e}")
            return None

    async def save_batch(self, logs: List[LLMBrainLog]) -> int:
        """
        è¤‡æ•°ã®ãƒ­ã‚°ã‚’ä¸€æ‹¬ä¿å­˜

        Args:
            logs: ãƒ­ã‚°ã‚¨ãƒ³ãƒˆãƒªã®ãƒªã‚¹ãƒˆ

        Returns:
            ä¿å­˜ã•ã‚ŒãŸä»¶æ•°
        """
        if not self.enable_persistence or not self.pool or not logs:
            return 0

        saved_count = 0
        for log in logs:
            if await self.save(log):
                saved_count += 1

        return saved_count

    def add_to_buffer(self, log_entry: LLMBrainLog) -> None:
        """
        ãƒãƒƒãƒ•ã‚¡ã«ãƒ­ã‚°ã‚’è¿½åŠ ï¼ˆãƒãƒƒãƒæŒ¿å…¥ç”¨ï¼‰

        Args:
            log_entry: ãƒ­ã‚°ã‚¨ãƒ³ãƒˆãƒª
        """
        self._buffer.append(log_entry)

        if len(self._buffer) >= self._buffer_max_size:
            # ãƒãƒƒãƒ•ã‚¡ãŒã„ã£ã±ã„ã«ãªã£ãŸã‚‰éåŒæœŸã§ä¿å­˜ã‚’ãƒˆãƒªã‚¬ãƒ¼
            # å®Ÿéš›ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯ asyncio.create_task() ã‚’ä½¿ç”¨
            logger.warning(
                f"Buffer full ({len(self._buffer)} logs), "
                "consider calling flush_buffer()"
            )

    async def flush_buffer(self) -> int:
        """
        ãƒãƒƒãƒ•ã‚¡ã®ãƒ­ã‚°ã‚’ä¸€æ‹¬ä¿å­˜

        Returns:
            ä¿å­˜ã•ã‚ŒãŸä»¶æ•°
        """
        if not self._buffer:
            return 0

        logs = self._buffer.copy()
        self._buffer.clear()

        return await self.save_batch(logs)


# =============================================================================
# ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
# =============================================================================

@dataclass
class UserFeedback:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯"""
    organization_id: str
    log_id: Optional[str]
    user_id: str
    rating: Optional[int] = None  # 1-5
    is_helpful: Optional[bool] = None
    feedback_type: Optional[str] = None
    comment: Optional[str] = None
    created_at: datetime = field(default_factory=datetime.utcnow)


async def save_user_feedback(
    pool,
    feedback: UserFeedback,
) -> Optional[str]:
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ä¿å­˜

    Args:
        pool: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šãƒ—ãƒ¼ãƒ«
        feedback: ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯

    Returns:
        ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã®ID
    """
    try:
        async with pool.acquire() as conn:
            result = await conn.fetchrow(
                """
                INSERT INTO brain_user_feedback (
                    organization_id, log_id, user_id,
                    rating, is_helpful, feedback_type, comment
                ) VALUES ($1, $2, $3, $4, $5, $6, $7)
                RETURNING id
                """,
                feedback.organization_id,
                feedback.log_id,
                feedback.user_id,
                feedback.rating,
                feedback.is_helpful,
                feedback.feedback_type,
                feedback.comment,
            )
            return str(result["id"]) if result else None
    except Exception as e:
        logger.error(f"Failed to save user feedback: {e}")
        return None


# =============================================================================
# ãƒ•ã‚¡ã‚¯ãƒˆãƒªé–¢æ•°
# =============================================================================

def create_llm_observability(
    pool=None,
    org_id: str = "",
    enable_cloud_logging: bool = True,
    enable_persistence: bool = True,
) -> LLMBrainObservability:
    """
    LLMBrainObservabilityã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ

    Args:
        pool: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šãƒ—ãƒ¼ãƒ«
        org_id: çµ„ç¹”ID
        enable_cloud_logging: Cloud Loggingã¸ã®å‡ºåŠ›ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã‹
        enable_persistence: DBã¸ã®æ°¸ç¶šåŒ–ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã‹

    Returns:
        LLMBrainObservability
    """
    return LLMBrainObservability(
        pool=pool,
        org_id=org_id,
        enable_cloud_logging=enable_cloud_logging,
        enable_persistence=enable_persistence,
    )


# =============================================================================
# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
# =============================================================================

_default_llm_observability: Optional[LLMBrainObservability] = None


def get_llm_observability(
    pool=None,
    org_id: str = "",
) -> LLMBrainObservability:
    """
    LLMBrainObservabilityã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—

    Args:
        pool: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šãƒ—ãƒ¼ãƒ«
        org_id: çµ„ç¹”ID

    Returns:
        LLMBrainObservability
    """
    global _default_llm_observability

    if _default_llm_observability is None:
        _default_llm_observability = create_llm_observability(
            pool=pool,
            org_id=org_id,
        )

    return _default_llm_observability
