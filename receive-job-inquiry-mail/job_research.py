"""
job_research.py - æ±‚äººåª’ä½“é€±æ¬¡ãƒªã‚µãƒ¼ãƒãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

Indeed Japan ã®å…¬é–‹ RSS ãƒ•ã‚£ãƒ¼ãƒ‰ã‚’ä½¿ã£ã¦ç«¶åˆæ±‚äººæƒ…å ±ã‚’å–å¾—ãƒ»åˆ†æã™ã‚‹ã€‚
API ç™»éŒ²ä¸è¦ãƒ»scraping ä¸è¦ãƒ»Indeed ã®å…¬å¼ RSS ä»•æ§˜ã«æº–æ‹ ã€‚

ã€å‹•ä½œã®æµã‚Œã€‘
  Indeed RSS â†’ æ±‚äººä¸€è¦§å–å¾— â†’ ä»¶æ•°/çµ¦ä¸/ä¼æ¥­ã‚’åˆ†æ â†’ ChatWork ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ

ã€æ¤œç´¢å¯¾è±¡ã‚«ãƒ†ã‚´ãƒªã€‘
  - ITãƒ»ã‚·ã‚¹ãƒ†ãƒ é–‹ç™ºï¼ˆSESãƒ»ITã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ï¼‰
  - ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
  - ã‚¤ãƒ³ãƒ•ãƒ©ãƒ»ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢
  - å–¶æ¥­ï¼ˆIT/SaaSï¼‰
"""

import logging
import urllib.parse
from dataclasses import dataclass, field
from datetime import datetime, timezone, timedelta
from typing import List, Optional

import defusedxml.ElementTree as ET  # XXE æ”»æ’ƒã‚’é˜²ãå®‰å…¨ãª XML ãƒ‘ãƒ¼ã‚µãƒ¼
from defusedxml import DefusedXmlException
import httpx

logger = logging.getLogger(__name__)

# =============================================================================
# æ¤œç´¢ã‚¯ã‚¨ãƒªå®šç¾©ï¼ˆsoulsyncs ãŒå¯¾è±¡ã¨ã™ã‚‹è·ç¨®ï¼‰
# =============================================================================

SEARCH_QUERIES = [
    {
        "label": "SESãƒ»ITã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢æ´¾é£",
        "q": "SES ITã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ æ´¾é£",
        "l": "æ±äº¬éƒ½",
    },
    {
        "label": "ã‚·ã‚¹ãƒ†ãƒ é–‹ç™ºã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢",
        "q": "ã‚·ã‚¹ãƒ†ãƒ é–‹ç™º ãƒ—ãƒ­ã‚°ãƒ©ãƒãƒ¼ æ­£ç¤¾å“¡",
        "l": "æ±äº¬éƒ½",
    },
    {
        "label": "ã‚¤ãƒ³ãƒ•ãƒ©ãƒ»ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢",
        "q": "ã‚¤ãƒ³ãƒ•ãƒ©ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ æ´¾é£",
        "l": "æ±äº¬éƒ½",
    },
    {
        "label": "ITãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼",
        "q": "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ IT PM æ´¾é£",
        "l": "æ±äº¬éƒ½",
    },
]

# Indeed Japan RSS ã®ãƒ™ãƒ¼ã‚¹ URL
_INDEED_RSS_BASE = "https://www.indeed.co.jp/rss"

# Indeed ç‹¬è‡ª XML åå‰ç©ºé–“ï¼ˆä»•æ§˜å¤‰æ›´ã«å‚™ãˆã¦ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ãã§ä½¿ç”¨ï¼‰
_NS = "https://www.indeed.com/about/rss"


def _find_ns(item: ET.Element, tag: str) -> Optional[ET.Element]:
    """
    åå‰ç©ºé–“ä»˜ãã‚¿ã‚°ã‚’æ¤œç´¢ã—ã€è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§
    åå‰ç©ºé–“ãªã—ã®ã‚¿ã‚°ã‚‚æ¤œç´¢ã™ã‚‹ï¼ˆIndeed RSS ã®ä»•æ§˜å¤‰æ›´ã«å¯¾å¿œï¼‰ã€‚
    """
    result = item.find(f"{{{_NS}}}{tag}")
    if result is None:
        result = item.find(tag)
    return result

# 1å›ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§å–å¾—ã™ã‚‹æœ€å¤§ä»¶æ•°
_MAX_RESULTS_PER_QUERY = 25


# =============================================================================
# ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
# =============================================================================

@dataclass
class JobPosting:
    """1ä»¶ã®æ±‚äººæƒ…å ±"""
    title: str
    company: str
    location: str
    salary: Optional[str] = None
    url: str = ""


@dataclass
class ResearchResult:
    """1ã‚«ãƒ†ã‚´ãƒªåˆ†ã®ãƒªã‚µãƒ¼ãƒçµæœ"""
    label: str
    query: str
    total_count: int
    sample_postings: List[JobPosting] = field(default_factory=list)
    salary_count: int = 0
    top_companies: List[str] = field(default_factory=list)


# =============================================================================
# Indeed RSS å–å¾—
# =============================================================================

def _fetch_indeed_rss(query: str, location: str, limit: int = _MAX_RESULTS_PER_QUERY) -> List[JobPosting]:
    """
    Indeed Japan ã® RSS ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰æ±‚äººæƒ…å ±ã‚’å–å¾—ã™ã‚‹ã€‚
    å–å¾—å¤±æ•—æ™‚ã¯ç©ºãƒªã‚¹ãƒˆã‚’è¿”ã™ï¼ˆä¾‹å¤–ã¯ä¸Šä½ã«ä¼æ’­ã•ã›ãªã„ï¼‰ã€‚
    """
    params = {
        "q": query,
        "l": location,
        "limit": str(limit),
        "sort": "date",
    }
    url = _INDEED_RSS_BASE + "?" + urllib.parse.urlencode(params)

    try:
        with httpx.Client(
            timeout=15.0,
            headers={"User-Agent": "Mozilla/5.0 (compatible; job-research-bot/1.0)"},
            follow_redirects=True,
        ) as client:
            resp = client.get(url)

        if resp.status_code != 200:
            logger.warning(f"Indeed RSS å–å¾—å¤±æ•— (status={resp.status_code}): {query}")
            return []

        # resp.contentï¼ˆbytesï¼‰ã‚’æ¸¡ã™ã“ã¨ã§ XML å®£è¨€ã® encoding æŒ‡å®šã¨ç«¶åˆã—ãªã„
        root = ET.fromstring(resp.content)
        channel = root.find("channel")
        if channel is None:
            logger.warning(f"Indeed RSS: channel ã‚¿ã‚°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {query}")
            return []

        postings: List[JobPosting] = []
        for item in channel.findall("item"):
            title = (item.findtext("title") or "").strip()
            if not title:
                continue

            # Indeed ç‹¬è‡ªåå‰ç©ºé–“ã‚¿ã‚°ï¼ˆåå‰ç©ºé–“å¤‰æ›´ã«å¯¾å¿œã—ãŸãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ãï¼‰
            company_tag = _find_ns(item, "company")
            company = (company_tag.text or "").strip() if company_tag is not None else ""

            city_tag = _find_ns(item, "city")
            state_tag = _find_ns(item, "state")
            location_parts = []
            if state_tag is not None and state_tag.text:
                location_parts.append(state_tag.text.strip())
            if city_tag is not None and city_tag.text:
                location_parts.append(city_tag.text.strip())
            loc = " ".join(location_parts) if location_parts else location

            salary_tag = _find_ns(item, "salary")
            salary = (salary_tag.text or "").strip() if salary_tag is not None else None
            if not salary:
                salary = None

            link = (item.findtext("link") or "").strip()

            postings.append(JobPosting(
                title=title,
                company=company,
                location=loc,
                salary=salary,
                url=link,
            ))

        return postings

    except (ET.ParseError, DefusedXmlException) as e:
        logger.warning(f"Indeed RSS è§£æã‚¨ãƒ©ãƒ¼: {e} (query={query})")
        return []
    except Exception as e:
        logger.error(f"Indeed RSS å–å¾—ä¾‹å¤–: {e} (query={query})")
        return []


# =============================================================================
# åˆ†æ
# =============================================================================

def _analyze(label: str, query: str, postings: List[JobPosting]) -> ResearchResult:
    """å–å¾—ã—ãŸæ±‚äººä¸€è¦§ã‚’åˆ†æã—ã¦ã‚µãƒãƒªãƒ¼ã‚’ä½œã‚‹"""

    # çµ¦ä¸è¨˜è¼‰ä»¶æ•°
    salary_count = sum(1 for p in postings if p.salary)

    # ä¸Šä½ä¼æ¥­ï¼ˆå‡ºç¾å›æ•°ãŒå¤šã„é †ï¼‰
    company_counts: dict[str, int] = {}
    for p in postings:
        if p.company:
            company_counts[p.company] = company_counts.get(p.company, 0) + 1
    top_companies = sorted(company_counts, key=lambda c: -company_counts[c])[:5]

    return ResearchResult(
        label=label,
        query=query,
        total_count=len(postings),
        sample_postings=postings[:5],
        salary_count=salary_count,
        top_companies=top_companies,
    )


# =============================================================================
# ChatWork ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”Ÿæˆ
# =============================================================================

def _build_chatwork_message(results: List[ResearchResult]) -> str:
    """ãƒªã‚µãƒ¼ãƒçµæœã‚’ ChatWork æŠ•ç¨¿ç”¨ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã™ã‚‹"""
    jst = timezone(timedelta(hours=9))
    today = datetime.now(jst).strftime("%Yå¹´%mæœˆ%dæ—¥")

    lines = [
        f"ğŸ“Šã€é€±æ¬¡æ±‚äººåª’ä½“ãƒªã‚µãƒ¼ãƒã€‘{today}",
        "Indeed Japan ã®ç›´è¿‘æ±‚äººçŠ¶æ³ã‚’ãŠå±Šã‘ã—ã¾ã™ï¼ˆæ¯é€±æœˆæ›œæ›´æ–°ï¼‰",
        "=" * 36,
    ]

    for r in results:
        lines.append(f"\nâ–  {r.label}")
        lines.append(f"  æ±‚äººä»¶æ•°: {r.total_count}ä»¶ï¼ˆç›´è¿‘/æ±äº¬éƒ½ï¼‰")

        if r.total_count > 0:
            salary_pct = int(r.salary_count / r.total_count * 100)
            lines.append(f"  çµ¦ä¸è¨˜è¼‰ã‚ã‚Š: {r.salary_count}ä»¶ï¼ˆ{salary_pct}%ï¼‰")

        if r.top_companies:
            companies_str = "ã€".join(r.top_companies[:3])
            lines.append(f"  æ²è¼‰ä¼æ¥­ï¼ˆä¸Šä½ï¼‰: {companies_str}")

        if r.sample_postings:
            lines.append("  ã€ç›´è¿‘ã®æ±‚äººã‚¿ã‚¤ãƒˆãƒ«ä¾‹ã€‘")
            for p in r.sample_postings[:3]:
                salary_str = f" [{p.salary}]" if p.salary else ""
                lines.append(f"  ãƒ»{p.title}{salary_str}")

    lines += [
        "",
        "=" * 36,
        "â€» Indeed Japan å…¬é–‹RSSï¼ˆæœ€æ–°é †ãƒ»æ±äº¬éƒ½ï¼‰ã‚ˆã‚Šè‡ªå‹•å–å¾—",
        "â€» ç«¶åˆçŠ¶æ³ã®å‚è€ƒã¨ã—ã¦ã”æ´»ç”¨ãã ã•ã„",
    ]

    return "\n".join(lines)


# =============================================================================
# å…¬é–‹ API
# =============================================================================

def run_weekly_research() -> Optional[str]:
    """
    é€±æ¬¡ãƒªã‚µãƒ¼ãƒã‚’å®Ÿè¡Œã—ã¦ ChatWork æŠ•ç¨¿ç”¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿”ã™ã€‚

    å–å¾—ã«å®Œå…¨å¤±æ•—ã—ãŸå ´åˆã®ã¿ None ã‚’è¿”ã™ã€‚
    ä¸€éƒ¨ã‚«ãƒ†ã‚´ãƒªã®å¤±æ•—ã¯ç„¡è¦–ã—ã¦æ®‹ã‚Šã®ãƒ‡ãƒ¼ã‚¿ã§å ±å‘Šã™ã‚‹ã€‚

    Returns:
        str: ChatWork æŠ•ç¨¿ç”¨ãƒ†ã‚­ã‚¹ãƒˆ
        None: ãƒ‡ãƒ¼ã‚¿ã‚’1ä»¶ã‚‚å–å¾—ã§ããªã‹ã£ãŸå ´åˆ
    """
    results: List[ResearchResult] = []

    for q in SEARCH_QUERIES:
        postings = _fetch_indeed_rss(q["q"], q["l"])
        if postings:
            r = _analyze(q["label"], q["q"], postings)
            results.append(r)
            logger.info(f"ãƒªã‚µãƒ¼ãƒå®Œäº†: {q['label']} â†’ {len(postings)}ä»¶")
        else:
            logger.warning(f"ãƒ‡ãƒ¼ã‚¿ãªã—: {q['label']}")

    if not results:
        logger.error("Indeed RSS ã‹ã‚‰æ±‚äººæƒ…å ±ã‚’1ä»¶ã‚‚å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return None

    return _build_chatwork_message(results)
