# lib/brain/monitoring.py
"""
LLM Brain ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

Task #9: æœ¬ç•ªãƒ­ã‚°åˆ†æãƒ»ã‚¨ãƒ©ãƒ¼ç‡ç¢ºèª

ã€ç›®çš„ã€‘
- LLM Brainã®æœ¬ç•ªç¨¼åƒçŠ¶æ³ã‚’ç›£è¦–
- ã‚¨ãƒ©ãƒ¼ç‡ã€ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã€APIã‚³ã‚¹ãƒˆã‚’è¿½è·¡
- ç•°å¸¸æ¤œçŸ¥ã¨è‡ªå‹•ã‚¢ãƒ©ãƒ¼ãƒˆ

ã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã€‘
- error_rate: ã‚¨ãƒ©ãƒ¼ç‡ï¼ˆç›®æ¨™: < 1%ï¼‰
- avg_response_time_ms: å¹³å‡ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ï¼ˆç›®æ¨™: < 3000msï¼‰
- api_cost_per_request: 1ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚ãŸã‚Šã®APIã‚³ã‚¹ãƒˆ
- confidence_distribution: ç¢ºä¿¡åº¦ã®åˆ†å¸ƒ
- guardian_block_rate: Guardian Layerã®ãƒ–ãƒ­ãƒƒã‚¯ç‡

Author: Claude Opus 4.5
Created: 2026-01-31
"""

import time
import logging
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from collections import defaultdict
from threading import Lock

logger = logging.getLogger(__name__)


# =============================================================================
# é–¾å€¤å®šç¾©
# =============================================================================

@dataclass
class MonitoringThresholds:
    """
    ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°é–¾å€¤

    è¨­è¨ˆæ›¸: docs/25_llm_native_brain_architecture.md ç¬¬15ç« ã€Œç›£è¦–ãƒ»é‹ç”¨ã€
    """
    # ã‚¨ãƒ©ãƒ¼ç‡ï¼ˆè¨­è¨ˆæ›¸: > 5% è­¦å‘Šï¼‰
    error_rate_warning: float = 0.01  # 1%ï¼ˆã‚ˆã‚Šæ—©ãæ¤œçŸ¥ï¼‰
    error_rate_critical: float = 0.05  # 5%ï¼ˆè¨­è¨ˆæ›¸æº–æ‹ ï¼‰

    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ï¼ˆè¨­è¨ˆæ›¸: > 10ç§’ è­¦å‘Šï¼‰
    response_time_warning_ms: int = 3000  # 3ç§’ï¼ˆã‚ˆã‚Šæ—©ãæ¤œçŸ¥ï¼‰
    response_time_critical_ms: int = 10000  # 10ç§’ï¼ˆè¨­è¨ˆæ›¸æº–æ‹ ï¼‰

    # APIã‚¨ãƒ©ãƒ¼ç‡
    api_error_rate_warning: float = 0.02  # 2%
    api_error_rate_critical: float = 0.10  # 10%

    # Guardian Layerãƒ–ãƒ­ãƒƒã‚¯ç‡ï¼ˆè¨­è¨ˆæ›¸: > 10% è­¦å‘Šï¼‰
    guardian_block_rate_warning: float = 0.10  # 10%ï¼ˆè¨­è¨ˆæ›¸æº–æ‹ ï¼‰
    guardian_block_rate_critical: float = 0.20  # 20%

    # ç¢ºèªãƒ¢ãƒ¼ãƒ‰ç™ºç”Ÿç‡ï¼ˆè¨­è¨ˆæ›¸: > 30% æƒ…å ±ï¼‰ã€v10.51.1è¿½åŠ ã€‘
    guardian_confirm_rate_info: float = 0.30  # 30%ï¼ˆè¨­è¨ˆæ›¸æº–æ‹ ï¼‰
    guardian_confirm_rate_warning: float = 0.50  # 50%

    # ç¢ºä¿¡åº¦
    low_confidence_threshold: float = 0.5  # 50%æœªæº€ã¯ä½ç¢ºä¿¡åº¦
    low_confidence_rate_warning: float = 0.20  # 20%ãŒä½ç¢ºä¿¡åº¦

    # ã‚³ã‚¹ãƒˆï¼ˆå††/ãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼‰
    cost_per_request_warning: float = 10.0  # 10å††
    cost_per_request_critical: float = 20.0  # 20å††

    # æ—¥æ¬¡ã‚³ã‚¹ãƒˆï¼ˆè¨­è¨ˆæ›¸: > 5,000å†† è­¦å‘Šï¼‰ã€v10.51.1è¿½åŠ ã€‘
    daily_cost_warning: float = 5000.0  # 5,000å††ï¼ˆè¨­è¨ˆæ›¸æº–æ‹ ï¼‰
    daily_cost_critical: float = 10000.0  # 10,000å††


# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé–¾å€¤
DEFAULT_THRESHOLDS = MonitoringThresholds()


# =============================================================================
# ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
# =============================================================================

@dataclass
class RequestMetrics:
    """1ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    request_id: str
    timestamp: datetime
    response_time_ms: int
    success: bool
    output_type: str
    confidence: float
    tool_name: Optional[str] = None
    guardian_action: str = "allow"
    api_provider: str = "openrouter"
    input_tokens: int = 0
    output_tokens: int = 0
    error_type: Optional[str] = None


@dataclass
class AggregatedMetrics:
    """é›†è¨ˆãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    period_start: datetime
    period_end: datetime

    # ã‚«ã‚¦ãƒ³ãƒˆ
    total_requests: int = 0
    successful_requests: int = 0
    failed_requests: int = 0
    api_errors: int = 0

    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“
    total_response_time_ms: int = 0
    max_response_time_ms: int = 0
    min_response_time_ms: int = 0

    # ç¢ºä¿¡åº¦
    total_confidence: float = 0.0
    low_confidence_count: int = 0

    # Guardian Layer
    guardian_allow_count: int = 0
    guardian_confirm_count: int = 0
    guardian_block_count: int = 0

    # ã‚³ã‚¹ãƒˆ
    total_input_tokens: int = 0
    total_output_tokens: int = 0

    # å‡ºåŠ›ã‚¿ã‚¤ãƒ—åˆ¥
    output_types: Dict[str, int] = field(default_factory=lambda: defaultdict(int))

    # ãƒ„ãƒ¼ãƒ«ä½¿ç”¨é »åº¦
    tools_used: Dict[str, int] = field(default_factory=lambda: defaultdict(int))

    # ã‚¨ãƒ©ãƒ¼ç¨®åˆ¥
    errors_by_type: Dict[str, int] = field(default_factory=lambda: defaultdict(int))

    @property
    def error_rate(self) -> float:
        """ã‚¨ãƒ©ãƒ¼ç‡"""
        if self.total_requests == 0:
            return 0.0
        return self.failed_requests / self.total_requests

    @property
    def avg_response_time_ms(self) -> float:
        """å¹³å‡ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“"""
        if self.total_requests == 0:
            return 0.0
        return self.total_response_time_ms / self.total_requests

    @property
    def avg_confidence(self) -> float:
        """å¹³å‡ç¢ºä¿¡åº¦"""
        if self.total_requests == 0:
            return 0.0
        return self.total_confidence / self.total_requests

    @property
    def guardian_block_rate(self) -> float:
        """Guardianãƒ–ãƒ­ãƒƒã‚¯ç‡"""
        total = self.guardian_allow_count + self.guardian_confirm_count + self.guardian_block_count
        if total == 0:
            return 0.0
        return self.guardian_block_count / total

    @property
    def guardian_confirm_rate(self) -> float:
        """Guardianç¢ºèªãƒ¢ãƒ¼ãƒ‰ç‡ï¼ˆè¨­è¨ˆæ›¸: > 30% æƒ…å ±ï¼‰"""
        total = self.guardian_allow_count + self.guardian_confirm_count + self.guardian_block_count
        if total == 0:
            return 0.0
        return self.guardian_confirm_count / total

    @property
    def low_confidence_rate(self) -> float:
        """ä½ç¢ºä¿¡åº¦ç‡"""
        if self.total_requests == 0:
            return 0.0
        return self.low_confidence_count / self.total_requests

    @property
    def estimated_cost_yen(self) -> float:
        """æ¨å®šã‚³ã‚¹ãƒˆï¼ˆå††ï¼‰"""
        # GPT-5.2 ã®æ–™é‡‘: $1.75/Må…¥åŠ›, $14/Må‡ºåŠ›
        # 1ãƒ‰ãƒ« = 154å††ã§è¨ˆç®—
        input_cost = (self.total_input_tokens / 1_000_000) * 1.75 * 154
        output_cost = (self.total_output_tokens / 1_000_000) * 14.0 * 154
        return input_cost + output_cost

    @property
    def cost_per_request(self) -> float:
        """1ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚ãŸã‚Šã®ã‚³ã‚¹ãƒˆ"""
        if self.total_requests == 0:
            return 0.0
        return self.estimated_cost_yen / self.total_requests

    def to_dict(self) -> Dict[str, Any]:
        """è¾æ›¸å½¢å¼ã«å¤‰æ›"""
        return {
            "period": {
                "start": self.period_start.isoformat(),
                "end": self.period_end.isoformat(),
            },
            "counts": {
                "total": self.total_requests,
                "successful": self.successful_requests,
                "failed": self.failed_requests,
                "api_errors": self.api_errors,
            },
            "rates": {
                "error_rate": round(self.error_rate, 4),
                "guardian_block_rate": round(self.guardian_block_rate, 4),
                "guardian_confirm_rate": round(self.guardian_confirm_rate, 4),
                "low_confidence_rate": round(self.low_confidence_rate, 4),
            },
            "response_time_ms": {
                "avg": round(self.avg_response_time_ms, 1),
                "max": self.max_response_time_ms,
                "min": self.min_response_time_ms,
            },
            "confidence": {
                "avg": round(self.avg_confidence, 3),
                "low_count": self.low_confidence_count,
            },
            "guardian": {
                "allow": self.guardian_allow_count,
                "confirm": self.guardian_confirm_count,
                "block": self.guardian_block_count,
            },
            "cost": {
                "total_yen": round(self.estimated_cost_yen, 2),
                "per_request_yen": round(self.cost_per_request, 2),
                "input_tokens": self.total_input_tokens,
                "output_tokens": self.total_output_tokens,
            },
            "output_types": dict(self.output_types),
            "top_tools": dict(sorted(
                self.tools_used.items(),
                key=lambda x: x[1],
                reverse=True
            )[:5]),
            "errors_by_type": dict(self.errors_by_type),
        }


# =============================================================================
# LLM Brain ãƒ¢ãƒ‹ã‚¿ãƒ¼
# =============================================================================

class LLMBrainMonitor:
    """
    LLM Brainã®ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°

    ä½¿ç”¨ä¾‹:
        monitor = LLMBrainMonitor()

        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–‹å§‹
        request_id = monitor.start_request()

        # å‡¦ç†å®Ÿè¡Œ...

        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Œäº†
        monitor.complete_request(
            request_id=request_id,
            success=True,
            output_type="tool_call",
            confidence=0.85,
            tool_name="chatwork_task_create",
            guardian_action="allow",
        )

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—
        metrics = monitor.get_current_metrics()
        print(f"ã‚¨ãƒ©ãƒ¼ç‡: {metrics.error_rate:.2%}")
    """

    def __init__(
        self,
        thresholds: MonitoringThresholds = DEFAULT_THRESHOLDS,
        aggregation_window_minutes: int = 5,
    ):
        """
        ãƒ¢ãƒ‹ã‚¿ãƒ¼ã‚’åˆæœŸåŒ–

        Args:
            thresholds: ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°é–¾å€¤
            aggregation_window_minutes: é›†è¨ˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼ˆåˆ†ï¼‰
        """
        self.thresholds = thresholds
        self.aggregation_window = timedelta(minutes=aggregation_window_minutes)

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¿å­˜
        self._requests: Dict[str, Dict[str, Any]] = {}
        self._completed_metrics: List[RequestMetrics] = []
        self._lock = Lock()

        # é›†è¨ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self._aggregated_cache: Optional[AggregatedMetrics] = None
        self._cache_time: Optional[datetime] = None
        self._cache_ttl = timedelta(seconds=30)

        logger.info(
            f"LLMBrainMonitor initialized: "
            f"window={aggregation_window_minutes}min"
        )

    def start_request(self, request_id: Optional[str] = None) -> str:
        """
        ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–‹å§‹ã‚’è¨˜éŒ²

        Args:
            request_id: ãƒªã‚¯ã‚¨ã‚¹ãƒˆIDï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯è‡ªå‹•ç”Ÿæˆï¼‰

        Returns:
            ãƒªã‚¯ã‚¨ã‚¹ãƒˆID
        """
        import uuid
        request_id = request_id or str(uuid.uuid4())[:8]

        with self._lock:
            self._requests[request_id] = {
                "start_time": time.time(),
                "timestamp": datetime.utcnow(),
            }

        return request_id

    def complete_request(
        self,
        request_id: str,
        success: bool,
        output_type: str = "text_response",
        confidence: float = 0.0,
        tool_name: Optional[str] = None,
        guardian_action: str = "allow",
        api_provider: str = "openrouter",
        input_tokens: int = 0,
        output_tokens: int = 0,
        error_type: Optional[str] = None,
    ) -> None:
        """
        ãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Œäº†ã‚’è¨˜éŒ²

        Args:
            request_id: ãƒªã‚¯ã‚¨ã‚¹ãƒˆID
            success: æˆåŠŸã—ãŸã‹
            output_type: å‡ºåŠ›ã‚¿ã‚¤ãƒ—
            confidence: ç¢ºä¿¡åº¦
            tool_name: ä½¿ç”¨ã—ãŸãƒ„ãƒ¼ãƒ«å
            guardian_action: Guardian Layerã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
            api_provider: ä½¿ç”¨ã—ãŸAPIãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼
            input_tokens: å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°
            output_tokens: å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°
            error_type: ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—ï¼ˆå¤±æ•—æ™‚ï¼‰
        """
        with self._lock:
            request_data = self._requests.pop(request_id, None)

            if request_data is None:
                # é–‹å§‹ãŒè¨˜éŒ²ã•ã‚Œã¦ã„ãªã„å ´åˆ
                response_time_ms = 0
                timestamp = datetime.utcnow()
            else:
                response_time_ms = int((time.time() - request_data["start_time"]) * 1000)
                timestamp = request_data["timestamp"]

            metrics = RequestMetrics(
                request_id=request_id,
                timestamp=timestamp,
                response_time_ms=response_time_ms,
                success=success,
                output_type=output_type,
                confidence=confidence,
                tool_name=tool_name,
                guardian_action=guardian_action,
                api_provider=api_provider,
                input_tokens=input_tokens,
                output_tokens=output_tokens,
                error_type=error_type,
            )

            self._completed_metrics.append(metrics)

            # å¤ã„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å‰Šé™¤ï¼ˆ1æ™‚é–“ä»¥ä¸Šå‰ï¼‰
            cutoff = datetime.utcnow() - timedelta(hours=1)
            self._completed_metrics = [
                m for m in self._completed_metrics
                if m.timestamp > cutoff
            ]

        # è­¦å‘Šãƒã‚§ãƒƒã‚¯
        self._check_alerts(metrics)

    def get_current_metrics(
        self,
        window_minutes: Optional[int] = None,
    ) -> AggregatedMetrics:
        """
        ç¾åœ¨ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—

        Args:
            window_minutes: é›†è¨ˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼ˆåˆ†ï¼‰

        Returns:
            é›†è¨ˆãƒ¡ãƒˆãƒªã‚¯ã‚¹
        """
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        if (
            self._aggregated_cache is not None
            and self._cache_time is not None
            and datetime.utcnow() - self._cache_time < self._cache_ttl
        ):
            return self._aggregated_cache

        window = timedelta(minutes=window_minutes) if window_minutes else self.aggregation_window
        now = datetime.utcnow()
        cutoff = now - window

        with self._lock:
            recent_metrics = [
                m for m in self._completed_metrics
                if m.timestamp > cutoff
            ]

        # é›†è¨ˆ
        aggregated = AggregatedMetrics(
            period_start=cutoff,
            period_end=now,
        )

        for m in recent_metrics:
            aggregated.total_requests += 1

            if m.success:
                aggregated.successful_requests += 1
            else:
                aggregated.failed_requests += 1
                if m.error_type == "api_error":
                    aggregated.api_errors += 1
                if m.error_type:
                    aggregated.errors_by_type[m.error_type] += 1

            aggregated.total_response_time_ms += m.response_time_ms
            aggregated.max_response_time_ms = max(
                aggregated.max_response_time_ms,
                m.response_time_ms
            )
            if aggregated.min_response_time_ms == 0 or m.response_time_ms < aggregated.min_response_time_ms:
                aggregated.min_response_time_ms = m.response_time_ms

            aggregated.total_confidence += m.confidence
            if m.confidence < self.thresholds.low_confidence_threshold:
                aggregated.low_confidence_count += 1

            if m.guardian_action == "allow":
                aggregated.guardian_allow_count += 1
            elif m.guardian_action == "confirm":
                aggregated.guardian_confirm_count += 1
            elif m.guardian_action == "block":
                aggregated.guardian_block_count += 1

            aggregated.total_input_tokens += m.input_tokens
            aggregated.total_output_tokens += m.output_tokens

            aggregated.output_types[m.output_type] += 1

            if m.tool_name:
                aggregated.tools_used[m.tool_name] += 1

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›´æ–°
        self._aggregated_cache = aggregated
        self._cache_time = now

        return aggregated

    def _check_alerts(self, metrics: RequestMetrics) -> None:
        """
        ã‚¢ãƒ©ãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯

        Args:
            metrics: å®Œäº†ã—ãŸãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        """
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã‚¢ãƒ©ãƒ¼ãƒˆ
        if metrics.response_time_ms > self.thresholds.response_time_critical_ms:
            logger.warning(
                f"CRITICAL: Response time exceeded threshold: "
                f"{metrics.response_time_ms}ms > {self.thresholds.response_time_critical_ms}ms"
            )
        elif metrics.response_time_ms > self.thresholds.response_time_warning_ms:
            logger.info(
                f"WARNING: Response time high: "
                f"{metrics.response_time_ms}ms > {self.thresholds.response_time_warning_ms}ms"
            )

        # ã‚¨ãƒ©ãƒ¼ã‚¢ãƒ©ãƒ¼ãƒˆ
        if not metrics.success and metrics.error_type == "api_error":
            logger.warning(f"API Error detected: {metrics.error_type}")

    def get_health_status(self) -> Dict[str, Any]:
        """
        ãƒ˜ãƒ«ã‚¹ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’å–å¾—

        Returns:
            ãƒ˜ãƒ«ã‚¹ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¾æ›¸
        """
        metrics = self.get_current_metrics()

        status = "healthy"
        issues = []

        # ã‚¨ãƒ©ãƒ¼ç‡ãƒã‚§ãƒƒã‚¯
        if metrics.error_rate >= self.thresholds.error_rate_critical:
            status = "critical"
            issues.append(f"Error rate critical: {metrics.error_rate:.2%}")
        elif metrics.error_rate >= self.thresholds.error_rate_warning:
            status = "warning"
            issues.append(f"Error rate high: {metrics.error_rate:.2%}")

        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ãƒã‚§ãƒƒã‚¯
        if metrics.avg_response_time_ms >= self.thresholds.response_time_critical_ms:
            status = "critical"
            issues.append(f"Response time critical: {metrics.avg_response_time_ms:.0f}ms")
        elif metrics.avg_response_time_ms >= self.thresholds.response_time_warning_ms:
            if status != "critical":
                status = "warning"
            issues.append(f"Response time high: {metrics.avg_response_time_ms:.0f}ms")

        # Guardian ãƒ–ãƒ­ãƒƒã‚¯ç‡ãƒã‚§ãƒƒã‚¯
        if metrics.guardian_block_rate >= self.thresholds.guardian_block_rate_critical:
            if status != "critical":
                status = "warning"
            issues.append(f"Guardian block rate high: {metrics.guardian_block_rate:.2%}")

        # Guardian ç¢ºèªãƒ¢ãƒ¼ãƒ‰ç‡ãƒã‚§ãƒƒã‚¯ï¼ˆè¨­è¨ˆæ›¸: > 30% æƒ…å ±ï¼‰
        if metrics.guardian_confirm_rate >= self.thresholds.guardian_confirm_rate_warning:
            if status not in ["critical", "warning"]:
                status = "warning"
            issues.append(f"Guardian confirm rate very high: {metrics.guardian_confirm_rate:.2%}")
        elif metrics.guardian_confirm_rate >= self.thresholds.guardian_confirm_rate_info:
            issues.append(f"Guardian confirm rate high (info): {metrics.guardian_confirm_rate:.2%}")

        return {
            "status": status,
            "issues": issues,
            "metrics": metrics.to_dict(),
            "checked_at": datetime.utcnow().isoformat(),
        }


# =============================================================================
# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
# =============================================================================

_monitor: Optional[LLMBrainMonitor] = None


def get_monitor() -> LLMBrainMonitor:
    """
    ãƒ¢ãƒ‹ã‚¿ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—ï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ï¼‰

    Returns:
        LLMBrainMonitor
    """
    global _monitor
    if _monitor is None:
        _monitor = LLMBrainMonitor()
    return _monitor


def start_request(request_id: Optional[str] = None) -> str:
    """ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–‹å§‹ã‚’è¨˜éŒ²ï¼ˆä¾¿åˆ©é–¢æ•°ï¼‰"""
    return get_monitor().start_request(request_id)


def complete_request(**kwargs) -> None:
    """ãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Œäº†ã‚’è¨˜éŒ²ï¼ˆä¾¿åˆ©é–¢æ•°ï¼‰"""
    get_monitor().complete_request(**kwargs)


def get_health_status() -> Dict[str, Any]:
    """ãƒ˜ãƒ«ã‚¹ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’å–å¾—ï¼ˆä¾¿åˆ©é–¢æ•°ï¼‰"""
    return get_monitor().get_health_status()


# =============================================================================
# ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“æœ€é©åŒ–ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# =============================================================================

class ResponseTimeOptimizer:
    """
    ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“æœ€é©åŒ–

    Task #10: ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“æœ€é©åŒ–

    ã€æœ€é©åŒ–é …ç›®ã€‘
    - ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆåŒä¸€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ»çŸ­æ™‚é–“å†…ã®é€£ç¶šãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼‰
    - ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®šï¼ˆå„ãƒ•ã‚§ãƒ¼ã‚ºã”ã¨ï¼‰
    - æ—©æœŸçµ‚äº†æ¡ä»¶ï¼ˆæ˜ç¢ºãªæ„å›³ã®å ´åˆï¼‰

    ã€ç›®æ¨™ã€‘
    - å¹³å‡ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“: 3ç§’ä»¥ä¸‹
    - 95ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«: 5ç§’ä»¥ä¸‹
    - ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: 30ç§’
    """

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥TTLï¼ˆç§’ï¼‰
    CONTEXT_CACHE_TTL_SECONDS = 30

    # ãƒ•ã‚§ãƒ¼ã‚ºåˆ¥ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰
    PHASE_TIMEOUTS = {
        "context_building": 5.0,
        "llm_processing": 20.0,
        "guardian_check": 2.0,
        "tool_execution": 10.0,
    }

    # æ—©æœŸçµ‚äº†ã®ç¢ºä¿¡åº¦é–¾å€¤
    EARLY_EXIT_CONFIDENCE = 0.95

    def __init__(self):
        self._context_cache: Dict[str, Dict[str, Any]] = {}
        self._cache_timestamps: Dict[str, float] = {}
        self._lock = Lock()

    def get_cached_context(
        self,
        user_id: str,
        room_id: str,
    ) -> Optional[Dict[str, Any]]:
        """
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—

        Args:
            user_id: ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
            room_id: ãƒ«ãƒ¼ãƒ ID

        Returns:
            ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆãªã‘ã‚Œã°Noneï¼‰
        """
        cache_key = f"{user_id}:{room_id}"

        with self._lock:
            if cache_key not in self._context_cache:
                return None

            timestamp = self._cache_timestamps.get(cache_key, 0)
            if time.time() - timestamp > self.CONTEXT_CACHE_TTL_SECONDS:
                # TTLè¶…é
                del self._context_cache[cache_key]
                del self._cache_timestamps[cache_key]
                return None

            return self._context_cache[cache_key]

    def set_cached_context(
        self,
        user_id: str,
        room_id: str,
        context: Dict[str, Any],
    ) -> None:
        """
        ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥

        Args:
            user_id: ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
            room_id: ãƒ«ãƒ¼ãƒ ID
            context: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        """
        cache_key = f"{user_id}:{room_id}"

        with self._lock:
            self._context_cache[cache_key] = context
            self._cache_timestamps[cache_key] = time.time()

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºåˆ¶é™ï¼ˆæœ€å¤§100ã‚¨ãƒ³ãƒˆãƒªï¼‰
            if len(self._context_cache) > 100:
                # æœ€å¤ã®ã‚¨ãƒ³ãƒˆãƒªã‚’å‰Šé™¤
                oldest_key = min(
                    self._cache_timestamps.keys(),
                    key=lambda k: self._cache_timestamps[k]
                )
                del self._context_cache[oldest_key]
                del self._cache_timestamps[oldest_key]

    def get_phase_timeout(self, phase: str) -> float:
        """
        ãƒ•ã‚§ãƒ¼ã‚ºåˆ¥ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’å–å¾—

        Args:
            phase: ãƒ•ã‚§ãƒ¼ã‚ºå

        Returns:
            ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç§’æ•°
        """
        return self.PHASE_TIMEOUTS.get(phase, 10.0)

    def should_early_exit(self, confidence: float) -> bool:
        """
        æ—©æœŸçµ‚äº†ã™ã¹ãã‹åˆ¤å®š

        Args:
            confidence: ç¢ºä¿¡åº¦

        Returns:
            æ—©æœŸçµ‚äº†ã™ã¹ããªã‚‰True
        """
        return confidence >= self.EARLY_EXIT_CONFIDENCE

    def invalidate_cache(self, user_id: str, room_id: str) -> None:
        """
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡åŠ¹åŒ–

        Args:
            user_id: ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
            room_id: ãƒ«ãƒ¼ãƒ ID
        """
        cache_key = f"{user_id}:{room_id}"

        with self._lock:
            if cache_key in self._context_cache:
                del self._context_cache[cache_key]
            if cache_key in self._cache_timestamps:
                del self._cache_timestamps[cache_key]


# ã‚°ãƒ­ãƒ¼ãƒãƒ«æœ€é©åŒ–ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_optimizer: Optional[ResponseTimeOptimizer] = None


def get_optimizer() -> ResponseTimeOptimizer:
    """æœ€é©åŒ–ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
    global _optimizer
    if _optimizer is None:
        _optimizer = ResponseTimeOptimizer()
    return _optimizer


# =============================================================================
# DBé€£æºãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ï¼ˆè¨­è¨ˆæ›¸15.2ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰å¯¾å¿œï¼‰
# =============================================================================

@dataclass
class DailyDBMetrics:
    """
    æ—¥æ¬¡DBãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆè¨­è¨ˆæ›¸15.2ã®ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”¨ï¼‰

    brain_daily_metrics ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰å–å¾—ã™ã‚‹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã€‚
    ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã® AggregatedMetrics ã¨ã¯ç•°ãªã‚Šã€DBã«æ°¸ç¶šåŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã€‚
    """
    # åŸºæœ¬æƒ…å ±
    organization_id: str
    metric_date: datetime

    # ä¼šè©±çµ±è¨ˆ
    total_conversations: int = 0
    unique_users: int = 0

    # å¿œç­”æ™‚é–“
    avg_response_time_ms: int = 0
    p50_response_time_ms: int = 0
    p95_response_time_ms: int = 0
    p99_response_time_ms: int = 0
    max_response_time_ms: int = 0

    # ç¢ºä¿¡åº¦
    avg_confidence: float = 0.0
    min_confidence: float = 0.0

    # å‡ºåŠ›ã‚¿ã‚¤ãƒ—åˆ¥
    tool_call_count: int = 0
    text_response_count: int = 0
    clarification_count: int = 0

    # Guardianåˆ¤å®š
    allow_count: int = 0
    confirm_count: int = 0
    block_count: int = 0

    # å®Ÿè¡Œçµæœ
    success_count: int = 0
    error_count: int = 0

    # ã‚³ã‚¹ãƒˆ
    total_input_tokens: int = 0
    total_output_tokens: int = 0
    total_cost_yen: float = 0.0

    # é…ã„ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
    slow_request_count: int = 0

    @property
    def error_rate(self) -> float:
        """ã‚¨ãƒ©ãƒ¼ç‡ï¼ˆ%ï¼‰"""
        if self.total_conversations == 0:
            return 0.0
        return (self.error_count / self.total_conversations) * 100

    @property
    def confirm_rate(self) -> float:
        """ç¢ºèªãƒ¢ãƒ¼ãƒ‰ç™ºç”Ÿç‡ï¼ˆ%ï¼‰"""
        if self.total_conversations == 0:
            return 0.0
        return (self.confirm_count / self.total_conversations) * 100

    @property
    def block_rate(self) -> float:
        """ãƒ–ãƒ­ãƒƒã‚¯ç™ºç”Ÿç‡ï¼ˆ%ï¼‰"""
        if self.total_conversations == 0:
            return 0.0
        return (self.block_count / self.total_conversations) * 100

    def check_alerts(self) -> List[Dict[str, Any]]:
        """
        è¨­è¨ˆæ›¸15.1ã®é–¾å€¤ã«åŸºã¥ãã‚¢ãƒ©ãƒ¼ãƒˆã‚’ãƒã‚§ãƒƒã‚¯

        Returns:
            ã‚¢ãƒ©ãƒ¼ãƒˆã®ãƒªã‚¹ãƒˆ
        """
        alerts = []
        thresholds = DEFAULT_THRESHOLDS

        # LLMå¿œç­”æ™‚é–“ > 10ç§’
        if self.avg_response_time_ms > thresholds.response_time_critical_ms:
            alerts.append({
                "level": "warning",
                "category": "response_time",
                "message": f"LLMå¿œç­”æ™‚é–“ãŒé–¾å€¤ã‚’è¶…ãˆã¦ã„ã¾ã™: {self.avg_response_time_ms}ms",
                "value": self.avg_response_time_ms,
                "threshold": thresholds.response_time_critical_ms,
            })

        # ã‚¨ãƒ©ãƒ¼ç‡ > 5%
        if self.error_rate > thresholds.error_rate_critical * 100:
            alerts.append({
                "level": "warning",
                "category": "error_rate",
                "message": f"ã‚¨ãƒ©ãƒ¼ç‡ãŒé–¾å€¤ã‚’è¶…ãˆã¦ã„ã¾ã™: {self.error_rate:.1f}%",
                "value": self.error_rate,
                "threshold": thresholds.error_rate_critical * 100,
            })

        # ç¢ºèªãƒ¢ãƒ¼ãƒ‰ç™ºç”Ÿç‡ > 30%
        if self.confirm_rate > thresholds.guardian_confirm_rate_info * 100:
            alerts.append({
                "level": "info",
                "category": "confirm_rate",
                "message": f"ç¢ºèªãƒ¢ãƒ¼ãƒ‰ç™ºç”Ÿç‡ãŒé«˜ããªã£ã¦ã„ã¾ã™: {self.confirm_rate:.1f}%",
                "value": self.confirm_rate,
                "threshold": thresholds.guardian_confirm_rate_info * 100,
            })

        # ãƒ–ãƒ­ãƒƒã‚¯ç™ºç”Ÿç‡ > 10%
        if self.block_rate > thresholds.guardian_block_rate_warning * 100:
            alerts.append({
                "level": "warning",
                "category": "block_rate",
                "message": f"ãƒ–ãƒ­ãƒƒã‚¯ç™ºç”Ÿç‡ãŒé–¾å€¤ã‚’è¶…ãˆã¦ã„ã¾ã™: {self.block_rate:.1f}%",
                "value": self.block_rate,
                "threshold": thresholds.guardian_block_rate_warning * 100,
            })

        # æ—¥æ¬¡ã‚³ã‚¹ãƒˆ > 5,000å††
        if self.total_cost_yen > thresholds.daily_cost_warning:
            alerts.append({
                "level": "warning",
                "category": "daily_cost",
                "message": f"æ—¥æ¬¡ã‚³ã‚¹ãƒˆãŒé–¾å€¤ã‚’è¶…ãˆã¦ã„ã¾ã™: Â¥{self.total_cost_yen:,.0f}",
                "value": self.total_cost_yen,
                "threshold": thresholds.daily_cost_warning,
            })

        return alerts

    def to_dashboard_string(self) -> str:
        """
        è¨­è¨ˆæ›¸15.2å½¢å¼ã®ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰æ–‡å­—åˆ—ã‚’ç”Ÿæˆ

        ã€ã‚½ã‚¦ãƒ«ãã‚“è„³ãƒ¢ãƒ‹ã‚¿ãƒ¼ã€‘
        ä»Šæ—¥ã®çµ±è¨ˆ:
        - ç·ä¼šè©±æ•°: 150
        - å¹³å‡å¿œç­”æ™‚é–“: 2.3ç§’
        - ç¢ºä¿¡åº¦å¹³å‡: 0.85
        - ç¢ºèªãƒ¢ãƒ¼ãƒ‰: 12å› (8%)
        - ãƒ–ãƒ­ãƒƒã‚¯: 2å› (1.3%)
        """
        avg_time_sec = self.avg_response_time_ms / 1000 if self.avg_response_time_ms else 0
        date_str = self.metric_date.strftime("%Y-%m-%d") if isinstance(self.metric_date, datetime) else str(self.metric_date)

        dashboard = f"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ã€ã‚½ã‚¦ãƒ«ãã‚“è„³ãƒ¢ãƒ‹ã‚¿ãƒ¼ã€‘ {date_str}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“Š ä»Šæ—¥ã®çµ±è¨ˆ:
  - ç·ä¼šè©±æ•°: {self.total_conversations:,}
  - ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ¦ãƒ¼ã‚¶ãƒ¼: {self.unique_users:,}
  - å¹³å‡å¿œç­”æ™‚é–“: {avg_time_sec:.1f}ç§’
  - ç¢ºä¿¡åº¦å¹³å‡: {self.avg_confidence:.2f}

ğŸ›¡ï¸ Guardianåˆ¤å®š:
  - è¨±å¯ (allow): {self.allow_count:,}å›
  - ç¢ºèª (confirm): {self.confirm_count:,}å› ({self.confirm_rate:.1f}%)
  - ãƒ–ãƒ­ãƒƒã‚¯ (block): {self.block_count:,}å› ({self.block_rate:.1f}%)

âœ… å®Ÿè¡Œçµæœ:
  - æˆåŠŸ: {self.success_count:,}å›
  - ã‚¨ãƒ©ãƒ¼: {self.error_count:,}å› ({self.error_rate:.1f}%)
  - é…å»¶ãƒªã‚¯ã‚¨ã‚¹ãƒˆ (>10ç§’): {self.slow_request_count:,}å›

ğŸ’° ã‚³ã‚¹ãƒˆ:
  - æœ¬æ—¥: Â¥{self.total_cost_yen:,.0f}
  - å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³: {self.total_input_tokens:,}
  - å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³: {self.total_output_tokens:,}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""
        # ã‚¢ãƒ©ãƒ¼ãƒˆãŒã‚ã‚Œã°è¿½åŠ 
        alerts = self.check_alerts()
        if alerts:
            dashboard += "\nğŸš¨ ã‚¢ãƒ©ãƒ¼ãƒˆ:\n"
            for alert in alerts:
                emoji = {"info": "â„¹ï¸", "warning": "âš ï¸", "critical": "ğŸš¨"}.get(alert["level"], "â“")
                dashboard += f"  {emoji} [{alert['level'].upper()}] {alert['message']}\n"
            dashboard += "\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"

        return dashboard


class DBBrainMonitor:
    """
    DBé€£æº Brain ãƒ¢ãƒ‹ã‚¿ãƒ¼

    brain_observability_logs ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—ã—ã¦
    è¨­è¨ˆæ›¸15.2å½¢å¼ã®ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚’è¡¨ç¤ºã™ã‚‹ã€‚

    Usage:
        monitor = DBBrainMonitor(pool=db_pool)

        # ä»Šæ—¥ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—
        metrics = await monitor.get_daily_metrics(org_id, date.today())
        print(metrics.to_dashboard_string())

        # ã‚¢ãƒ©ãƒ¼ãƒˆã‚’ãƒã‚§ãƒƒã‚¯
        alerts = metrics.check_alerts()
    """

    def __init__(self, pool=None):
        """
        åˆæœŸåŒ–

        Args:
            pool: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šãƒ—ãƒ¼ãƒ«ï¼ˆasyncpgï¼‰
        """
        self.pool = pool

    async def get_daily_metrics(
        self,
        organization_id: str,
        target_date: datetime,
    ) -> DailyDBMetrics:
        """
        æ—¥æ¬¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—

        Args:
            organization_id: çµ„ç¹”ID
            target_date: å¯¾è±¡æ—¥

        Returns:
            DailyDBMetrics
        """
        if not self.pool:
            logger.warning("No pool available, returning empty metrics")
            return DailyDBMetrics(
                organization_id=organization_id,
                metric_date=target_date,
            )

        try:
            async with self.pool.acquire() as conn:
                # ã¾ãšé›†è¨ˆãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ç¢ºèª
                row = await conn.fetchrow(
                    """
                    SELECT * FROM brain_daily_metrics
                    WHERE organization_id = $1 AND metric_date = $2
                    """,
                    organization_id,
                    target_date.date() if isinstance(target_date, datetime) else target_date,
                )

                if row:
                    return self._row_to_metrics(row, organization_id, target_date)

                # ãªã‘ã‚Œã°ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é›†è¨ˆ
                return await self._aggregate_realtime(conn, organization_id, target_date)

        except Exception as e:
            logger.error(f"Failed to get daily metrics: {e}")
            return DailyDBMetrics(
                organization_id=organization_id,
                metric_date=target_date,
            )

    async def _aggregate_realtime(
        self,
        conn,
        organization_id: str,
        target_date: datetime,
    ) -> DailyDBMetrics:
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é›†è¨ˆ"""
        date_value = target_date.date() if isinstance(target_date, datetime) else target_date

        row = await conn.fetchrow(
            """
            SELECT
                COUNT(*) as total_conversations,
                COUNT(DISTINCT user_id) as unique_users,
                COALESCE(AVG(total_response_time_ms)::INTEGER, 0) as avg_response_time_ms,
                COALESCE(PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY total_response_time_ms)::INTEGER, 0) as p50_response_time_ms,
                COALESCE(PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY total_response_time_ms)::INTEGER, 0) as p95_response_time_ms,
                COALESCE(PERCENTILE_CONT(0.99) WITHIN GROUP (ORDER BY total_response_time_ms)::INTEGER, 0) as p99_response_time_ms,
                COALESCE(MAX(total_response_time_ms), 0) as max_response_time_ms,
                COALESCE(AVG(confidence_overall), 0) as avg_confidence,
                COALESCE(MIN(confidence_overall), 0) as min_confidence,
                COUNT(*) FILTER (WHERE output_type = 'tool_call') as tool_call_count,
                COUNT(*) FILTER (WHERE output_type = 'text_response') as text_response_count,
                COUNT(*) FILTER (WHERE output_type = 'clarification_needed') as clarification_count,
                COUNT(*) FILTER (WHERE guardian_action = 'allow') as allow_count,
                COUNT(*) FILTER (WHERE guardian_action = 'confirm') as confirm_count,
                COUNT(*) FILTER (WHERE guardian_action = 'block') as block_count,
                COUNT(*) FILTER (WHERE execution_success = TRUE) as success_count,
                COUNT(*) FILTER (WHERE execution_success = FALSE) as error_count,
                COALESCE(SUM(input_tokens), 0) as total_input_tokens,
                COALESCE(SUM(output_tokens), 0) as total_output_tokens,
                COALESCE(SUM(estimated_cost_yen), 0) as total_cost_yen,
                COUNT(*) FILTER (WHERE total_response_time_ms > 10000) as slow_request_count
            FROM brain_observability_logs
            WHERE organization_id = $1
              AND DATE(created_at) = $2
              AND environment = 'production'
            """,
            organization_id,
            date_value,
        )

        if not row:
            return DailyDBMetrics(
                organization_id=organization_id,
                metric_date=target_date,
            )

        return DailyDBMetrics(
            organization_id=organization_id,
            metric_date=target_date,
            total_conversations=row["total_conversations"] or 0,
            unique_users=row["unique_users"] or 0,
            avg_response_time_ms=row["avg_response_time_ms"] or 0,
            p50_response_time_ms=row["p50_response_time_ms"] or 0,
            p95_response_time_ms=row["p95_response_time_ms"] or 0,
            p99_response_time_ms=row["p99_response_time_ms"] or 0,
            max_response_time_ms=row["max_response_time_ms"] or 0,
            avg_confidence=float(row["avg_confidence"]) if row["avg_confidence"] else 0.0,
            min_confidence=float(row["min_confidence"]) if row["min_confidence"] else 0.0,
            tool_call_count=row["tool_call_count"] or 0,
            text_response_count=row["text_response_count"] or 0,
            clarification_count=row["clarification_count"] or 0,
            allow_count=row["allow_count"] or 0,
            confirm_count=row["confirm_count"] or 0,
            block_count=row["block_count"] or 0,
            success_count=row["success_count"] or 0,
            error_count=row["error_count"] or 0,
            total_input_tokens=row["total_input_tokens"] or 0,
            total_output_tokens=row["total_output_tokens"] or 0,
            total_cost_yen=float(row["total_cost_yen"] or 0),
            slow_request_count=row["slow_request_count"] or 0,
        )

    def _row_to_metrics(
        self,
        row,
        organization_id: str,
        target_date: datetime,
    ) -> DailyDBMetrics:
        """DBã®è¡Œã‚’DailyDBMetricsã«å¤‰æ›"""
        return DailyDBMetrics(
            organization_id=organization_id,
            metric_date=target_date,
            total_conversations=row["total_conversations"] or 0,
            unique_users=row["unique_users"] or 0,
            avg_response_time_ms=row["avg_response_time_ms"] or 0,
            p50_response_time_ms=row["p50_response_time_ms"] or 0,
            p95_response_time_ms=row["p95_response_time_ms"] or 0,
            p99_response_time_ms=row["p99_response_time_ms"] or 0,
            max_response_time_ms=row["max_response_time_ms"] or 0,
            avg_confidence=float(row["avg_confidence"]) if row["avg_confidence"] else 0.0,
            min_confidence=float(row["min_confidence"]) if row["min_confidence"] else 0.0,
            tool_call_count=row["tool_call_count"] or 0,
            text_response_count=row["text_response_count"] or 0,
            clarification_count=row["clarification_count"] or 0,
            allow_count=row["allow_count"] or 0,
            confirm_count=row["confirm_count"] or 0,
            block_count=row["block_count"] or 0,
            success_count=row["success_count"] or 0,
            error_count=row["error_count"] or 0,
            total_input_tokens=row["total_input_tokens"] or 0,
            total_output_tokens=row["total_output_tokens"] or 0,
            total_cost_yen=float(row["total_cost_yen"] or 0),
            slow_request_count=row["slow_request_count"] or 0,
        )

    async def get_monthly_cost(
        self,
        organization_id: str,
        year: int,
        month: int,
    ) -> Dict[str, Any]:
        """
        æœˆæ¬¡ã‚³ã‚¹ãƒˆã‚’å–å¾—

        Args:
            organization_id: çµ„ç¹”ID
            year: å¹´
            month: æœˆ

        Returns:
            æœˆæ¬¡ã‚³ã‚¹ãƒˆæƒ…å ±
        """
        if not self.pool:
            return {"error": "No pool available"}

        try:
            async with self.pool.acquire() as conn:
                row = await conn.fetchrow(
                    """
                    SELECT
                        COUNT(*) as total_conversations,
                        COALESCE(SUM(input_tokens), 0) as total_input_tokens,
                        COALESCE(SUM(output_tokens), 0) as total_output_tokens,
                        COALESCE(SUM(estimated_cost_yen), 0) as total_cost_yen,
                        COALESCE(AVG(estimated_cost_yen), 0) as avg_cost_per_conversation
                    FROM brain_observability_logs
                    WHERE organization_id = $1
                      AND EXTRACT(YEAR FROM created_at) = $2
                      AND EXTRACT(MONTH FROM created_at) = $3
                      AND environment = 'production'
                    """,
                    organization_id,
                    year,
                    month,
                )

                return {
                    "year": year,
                    "month": month,
                    "total_conversations": row["total_conversations"] or 0,
                    "total_input_tokens": row["total_input_tokens"] or 0,
                    "total_output_tokens": row["total_output_tokens"] or 0,
                    "total_cost_yen": float(row["total_cost_yen"] or 0),
                    "avg_cost_per_conversation": float(row["avg_cost_per_conversation"] or 0),
                }

        except Exception as e:
            logger.error(f"Failed to get monthly cost: {e}")
            return {"error": str(e)}

    async def trigger_daily_aggregation(
        self,
        organization_id: str,
        target_date: datetime,
    ) -> bool:
        """
        æ—¥æ¬¡é›†è¨ˆã‚’ãƒˆãƒªã‚¬ãƒ¼

        Args:
            organization_id: çµ„ç¹”ID
            target_date: å¯¾è±¡æ—¥

        Returns:
            æˆåŠŸã—ãŸã‹
        """
        if not self.pool:
            return False

        try:
            date_value = target_date.date() if isinstance(target_date, datetime) else target_date

            async with self.pool.acquire() as conn:
                await conn.execute(
                    "SELECT aggregate_brain_daily_metrics($1, $2)",
                    organization_id,
                    date_value,
                )
                logger.info(f"Daily aggregation completed: {organization_id} {date_value}")
                return True

        except Exception as e:
            logger.error(f"Failed to trigger daily aggregation: {e}")
            return False


# DBãƒ¢ãƒ‹ã‚¿ãƒ¼ã®ãƒ•ã‚¡ã‚¯ãƒˆãƒªé–¢æ•°
def create_db_monitor(pool=None) -> DBBrainMonitor:
    """
    DBBrainMonitorã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ

    Args:
        pool: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šãƒ—ãƒ¼ãƒ«

    Returns:
        DBBrainMonitor
    """
    return DBBrainMonitor(pool=pool)


# =============================================================================
# CLI ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# =============================================================================

async def print_dashboard(pool, organization_id: str, target_date: Optional[datetime] = None):
    """
    ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚’è¡¨ç¤ºï¼ˆCLIç”¨ï¼‰

    Args:
        pool: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šãƒ—ãƒ¼ãƒ«
        organization_id: çµ„ç¹”ID
        target_date: å¯¾è±¡æ—¥ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ä»Šæ—¥ï¼‰
    """
    if target_date is None:
        target_date = datetime.utcnow()

    monitor = DBBrainMonitor(pool=pool)
    metrics = await monitor.get_daily_metrics(organization_id, target_date)
    print(metrics.to_dashboard_string())
