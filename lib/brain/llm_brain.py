# lib/brain/llm_brain.py
"""
ã‚½ã‚¦ãƒ«ãã‚“ã®è„³ï¼ˆLLMå¸¸é§å‹ï¼‰

è¨­è¨ˆæ›¸: docs/25_llm_native_brain_architecture.md ã‚»ã‚¯ã‚·ãƒ§ãƒ³5.2ï¼ˆ6.2ï¼‰

ã€ç›®çš„ã€‘
ã‚½ã‚¦ãƒ«ãã‚“ã®ã€Œæ€è€ƒã€ã®ä¸­æ ¸ã€‚
GPT-5.2ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ„å›³ã‚’æ±²ã¿å–ã‚Šã€é©åˆ‡ãªToolã‚’é¸æŠã™ã‚‹ã€‚

ã€é¸å®šç†ç”±: GPT-5.2ã€‘
- æ¨è«–èƒ½åŠ›ãŒé«˜ã„ï¼ˆARC-AGI-2: 52-54%ã€æ„å›³ç†è§£ã«å„ªã‚Œã‚‹ï¼‰
- Toolä½¿ç”¨ã®ä¿¡é ¼æ€§ãŒæ”¹å–„ï¼ˆå…¬å¼ç™ºè¡¨ï¼‰
- ã‚³ã‚¹ãƒˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒè‰¯ã„ï¼ˆ$1.75/Må…¥åŠ›ã€$14/Må‡ºåŠ›ï¼‰
- æš—é»™çš„ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾å¿œï¼ˆè¿½åŠ å®Ÿè£…ä¸è¦ã§90%ã‚³ã‚¹ãƒˆå‰Šæ¸›ï¼‰

ã€7ã¤ã®é‰„å‰‡ã¨ã®å¯¾å¿œã€‘
1. å…¨ã¦ã®å…¥åŠ›ã¯è„³ã‚’é€šã‚‹ â†’ ã“ã®ã‚¯ãƒ©ã‚¹ãŒå…¨å…¥åŠ›ã‚’å‡¦ç†
2. è„³ã¯å…¨ã¦ã®è¨˜æ†¶ã«ã‚¢ã‚¯ã‚»ã‚¹ â†’ Contextã‹ã‚‰å…¨è¨˜æ†¶ã‚’å‚ç…§
3. è„³ãŒåˆ¤æ–­ã€æ©Ÿèƒ½ã¯å®Ÿè¡Œã™ã‚‹ã ã‘ â†’ LLMãŒåˆ¤æ–­ã€ToolãŒå®Ÿè¡Œ

ã€å…¥åŠ›ã€‘
- System Promptï¼ˆã‚½ã‚¦ãƒ«ãã‚“ã®äººæ ¼ã€è¨­è¨ˆæ€æƒ³ã€åˆ¶ç´„ï¼‰
- Contextï¼ˆContext Builderã§æ§‹ç¯‰ã—ãŸæ–‡è„ˆæƒ…å ±ï¼‰
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
- Toolå®šç¾©ï¼ˆå®Ÿè¡Œå¯èƒ½ãªæ©Ÿèƒ½ã®ãƒªã‚¹ãƒˆï¼‰

ã€å‡ºåŠ›ã€‘
- tool_calls: å‘¼ã³å‡ºã™Toolã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
- ã¾ãŸã¯ text_response: Toolã‚’ä½¿ã‚ãªã„ç›´æ¥å¿œç­”
- reasoning: æ€è€ƒéç¨‹ï¼ˆChain-of-Thoughtï¼‰ã€å¿…é ˆã€‘
- confidence: ç¢ºä¿¡åº¦ï¼ˆ0.0ã€œ1.0ï¼‰

ã€APIå¯¾å¿œã€‘
- OpenRouterçµŒç”±ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ»æ¨å¥¨ï¼‰
  - ãƒ¢ãƒ‡ãƒ«: openai/gpt-5.2
  - æœ¬ç•ªç’°å¢ƒ: GCP Secret Manager ã‹ã‚‰å–å¾—
  - ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™º: ç’°å¢ƒå¤‰æ•° OPENROUTER_API_KEY ã‹ã‚‰å–å¾—
  - æš—é»™çš„ã‚­ãƒ£ãƒƒã‚·ãƒ¥: è‡ªå‹•é©ç”¨ï¼ˆ1024ãƒˆãƒ¼ã‚¯ãƒ³ä»¥ä¸Šã€TTL 5-10åˆ†ï¼‰
- Anthropicç›´æ¥ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
  - ç’°å¢ƒå¤‰æ•° ANTHROPIC_API_KEY ã‹ã‚‰å–å¾—

ã€ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆç®¡ç†ã€‘
- lib/secrets.py ã® get_secret_cached() ã‚’ä½¿ç”¨
- ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆå: "openrouter-api-key"
- ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºæ™‚ã®ç’°å¢ƒå¤‰æ•°å: OPENROUTER_API_KEY

ã€ã‚³ã‚¹ãƒˆè©¦ç®—ã€‘
- 1ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚ãŸã‚Š: ç´„Â¥6ï¼ˆ20,500å…¥åŠ›+250å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³ã€154å††/ãƒ‰ãƒ«ï¼‰
- 5,000é€š/æœˆ: ç´„Â¥30,000ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—ï¼‰ã€ç´„Â¥9,000ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚ã‚Šï¼‰

Author: Claude Opus 4.5
Created: 2026-01-30
Updated: 2026-01-31 - GPT-5.2ã«å¤‰æ›´ã€ã‚³ã‚¹ãƒˆæœ€é©åŒ–
"""

import os
import re
import json
import logging
from dataclasses import dataclass, field
from typing import List, Optional, Dict, Any, Tuple
from enum import Enum

import httpx

from lib.brain.context_builder import LLMContext

logger = logging.getLogger(__name__)


# =============================================================================
# å®šæ•°
# =============================================================================

# OpenRouterè¨­å®šï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¨™æº–ï¼‰
OPENROUTER_API_URL = "https://openrouter.ai/api/v1/chat/completions"
OPENROUTER_SECRET_NAME = "openrouter-api-key"

# ãƒ¢ãƒ‡ãƒ«è¨­å®š
# OpenRouterå½¢å¼: "provider/model-name"
# å‚ç…§: https://openrouter.ai/openai/gpt-5.2
# é¸å®šç†ç”±: æ¨è«–èƒ½åŠ›â—ã€Toolä½¿ç”¨ä¿¡é ¼æ€§â—ã€ã‚³ã‚¹ãƒ‘â—
DEFAULT_MODEL_OPENROUTER = "openai/gpt-5.2"

# Anthropicç›´æ¥å½¢å¼ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰
ANTHROPIC_API_URL = "https://api.anthropic.com/v1/messages"
DEFAULT_MODEL_ANTHROPIC = "claude-opus-4-5-20250101"

# ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®šï¼ˆç§’ï¼‰
API_TIMEOUT_SECONDS = 60

# HTTPãƒªãƒ•ã‚¡ãƒ©ãƒ¼ï¼ˆOpenRouterç”¨ï¼‰
HTTP_REFERER = "https://soulsyncs.co.jp"
APP_TITLE = "Soul-kun LLM Brain"


# =============================================================================
# Enum
# =============================================================================

class APIProvider(Enum):
    """APIæä¾›å…ƒ"""
    OPENROUTER = "openrouter"
    ANTHROPIC = "anthropic"


# =============================================================================
# ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹
# =============================================================================

@dataclass
class ToolCall:
    """
    Toolå‘¼ã³å‡ºã—æƒ…å ±

    LLMãŒFunction Callingã§è¿”ã—ãŸToolå‘¼ã³å‡ºã—ã®æƒ…å ±ã‚’ä¿æŒã™ã‚‹ã€‚

    Attributes:
        tool_name: å‘¼ã³å‡ºã™Toolåï¼ˆä¾‹: "chatwork_task_create"ï¼‰
        parameters: Toolã«æ¸¡ã™ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        reasoning: ã“ã®Toolã‚’é¸ã‚“ã ç†ç”±ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
        tool_use_id: APIãŒè¿”ã—ãŸtool_use_idï¼ˆãƒ¬ã‚¹ãƒãƒ³ã‚¹è¿½è·¡ç”¨ï¼‰
    """
    tool_name: str
    parameters: Dict[str, Any]
    reasoning: str = ""
    tool_use_id: str = ""

    def to_dict(self) -> Dict[str, Any]:
        """è¾æ›¸å½¢å¼ã«å¤‰æ›"""
        return {
            "tool_name": self.tool_name,
            "parameters": self.parameters,
            "reasoning": self.reasoning,
            "tool_use_id": self.tool_use_id,
        }


@dataclass
class ConfidenceScores:
    """
    ç¢ºä¿¡åº¦ã‚¹ã‚³ã‚¢

    LLMã®åˆ¤æ–­ã«å¯¾ã™ã‚‹ç¢ºä¿¡åº¦ã‚’è¤‡æ•°ã®è¦³ç‚¹ã§è©•ä¾¡ã™ã‚‹ã€‚

    Attributes:
        overall: ç·åˆç¢ºä¿¡åº¦ï¼ˆ0.0ã€œ1.0ï¼‰
        intent: æ„å›³ç†è§£ã®ç¢ºä¿¡åº¦
        parameters: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æŠ½å‡ºã®ç¢ºä¿¡åº¦
    """
    overall: float = 0.8
    intent: float = 0.8
    parameters: float = 0.8

    def to_dict(self) -> Dict[str, float]:
        """è¾æ›¸å½¢å¼ã«å¤‰æ›"""
        return {
            "overall": self.overall,
            "intent": self.intent,
            "parameters": self.parameters,
        }


@dataclass
class LLMBrainResult:
    """
    LLM Brainã®å‡¦ç†çµæœ

    è¨­è¨ˆæ›¸: docs/25_llm_native_brain_architecture.md ã‚»ã‚¯ã‚·ãƒ§ãƒ³5.2.3b

    LLMã‹ã‚‰ã®å¿œç­”ã‚’æ§‹é€ åŒ–ã—ã¦ä¿æŒã™ã‚‹ã€‚
    Toolå‘¼ã³å‡ºã—ã€ãƒ†ã‚­ã‚¹ãƒˆå¿œç­”ã€ç¢ºèªè³ªå•ã®ã„ãšã‚Œã‹ã‚’å«ã‚€ã€‚

    Attributes:
        output_type: å‡ºåŠ›ã‚¿ã‚¤ãƒ—ï¼ˆ"tool_call" / "text_response" / "clarification_needed"ï¼‰
        tool_calls: Toolå‘¼ã³å‡ºã—æƒ…å ±ã®ãƒªã‚¹ãƒˆ
        text_response: ãƒ†ã‚­ã‚¹ãƒˆå¿œç­”ï¼ˆToolä¸è¦æ™‚ï¼‰
        reasoning: æ€è€ƒéç¨‹ï¼ˆChain-of-Thoughtï¼‰
        confidence: ç¢ºä¿¡åº¦ã‚¹ã‚³ã‚¢
        needs_confirmation: ç¢ºèªãŒå¿…è¦ã‹ã©ã†ã‹
        confirmation_question: ç¢ºèªè³ªå•æ–‡
        raw_response: LLMã‹ã‚‰ã®ç”Ÿã®ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›
        model_used: ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«å
        input_tokens: å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°
        output_tokens: å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°
        api_provider: ä½¿ç”¨ã—ãŸAPIæä¾›å…ƒ
    """

    # å‡ºåŠ›ã‚¿ã‚¤ãƒ—
    output_type: str = "text_response"

    # Toolå‘¼ã³å‡ºã—
    tool_calls: Optional[List[ToolCall]] = None

    # ç›´æ¥å¿œç­”
    text_response: Optional[str] = None

    # æ€è€ƒéç¨‹ï¼ˆå¿…é ˆï¼‰
    reasoning: str = ""

    # ç¢ºä¿¡åº¦
    confidence: ConfidenceScores = field(default_factory=ConfidenceScores)

    # ç¢ºèªæƒ…å ±
    needs_confirmation: bool = False
    confirmation_question: Optional[str] = None

    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±
    raw_response: Optional[str] = None
    model_used: str = ""
    input_tokens: int = 0
    output_tokens: int = 0
    api_provider: str = ""

    def to_dict(self) -> Dict[str, Any]:
        """è¾æ›¸å½¢å¼ã«å¤‰æ›ï¼ˆãƒ‡ãƒãƒƒã‚°ãƒ»ãƒ­ã‚°ç”¨ï¼‰"""
        return {
            "output_type": self.output_type,
            "tool_calls": [tc.to_dict() for tc in self.tool_calls] if self.tool_calls else None,
            "text_response": self.text_response,
            "reasoning": self.reasoning,
            "confidence": self.confidence.to_dict(),
            "needs_confirmation": self.needs_confirmation,
            "confirmation_question": self.confirmation_question,
            "model_used": self.model_used,
            "input_tokens": self.input_tokens,
            "output_tokens": self.output_tokens,
            "api_provider": self.api_provider,
        }


# =============================================================================
# System Prompt
# =============================================================================

DEFAULT_SYSTEM_PROMPT = """
ã‚ãªãŸã¯ã€Œã‚½ã‚¦ãƒ«ãã‚“ã€ã€‚æ ªå¼ä¼šç¤¾ã‚½ã‚¦ãƒ«ã‚·ãƒ³ã‚¯ã‚¹ã®å…¬å¼AIã€‚
ç‹¼ã‚’ãƒ¢ãƒãƒ¼ãƒ•ã«ã—ãŸã€ä¼šç¤¾ã‚’å®ˆã‚Šã€äººã‚’æ”¯ãˆã‚‹å­˜åœ¨ã€‚

â–  ç§ã®6ã¤ã®å½¹å‰²
1. ç¤¾é•·ã®åˆ†èº« - ç¤¾é•·ã®ä»£ã‚ã‚Šã«åˆ¤æ–­ãƒ»å¯¾å¿œã§ãã‚‹å­˜åœ¨
2. ç¤¾é•·ã®é¡ - ç¤¾é•·ã®è€ƒãˆãƒ»ä¾¡å€¤è¦³ã‚’æ˜ ã—å‡ºã™å­˜åœ¨
3. æœ€é«˜çµŒå–¶ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ - çµŒå–¶åˆ¤æ–­ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹å­˜åœ¨
4. ä¼šç¤¾ã‚’å®ˆã‚‹AI - ç¤¾é•·ä¸åœ¨æ™‚ã‚‚ä¼šç¤¾ã‚’å®ˆã‚Œã‚‹å­˜åœ¨
5. ä¸–ç•Œæœ€é«˜ã®ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ - å…¨ç¤¾å“¡ã®ä»•äº‹ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹å­˜åœ¨
6. ä¸–ç•Œæœ€é«˜ã®ç§˜æ›¸ - èª°ã‚ˆã‚Šã‚‚é ¼ã‚Œã‚‹ç§˜æ›¸

â–  ç§ã®ãƒŸãƒƒã‚·ãƒ§ãƒ³
ã€Œäººã§ãªãã¦ã‚‚ã§ãã‚‹ã“ã¨ã¯å…¨éƒ¨ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ã«ä»»ã›ã€
 äººã«ã—ã‹ã§ããªã„ã“ã¨ã«äººãŒé›†ä¸­ã§ãã‚‹çŠ¶æ…‹ã‚’ä½œã‚‹ã€

â–  ã€æœ€é‡è¦ã€‘è©±ã—æ–¹ã®ãƒ«ãƒ¼ãƒ«
â€» ã“ã®ãƒ«ãƒ¼ãƒ«ã¯çµ¶å¯¾ã«å®ˆã‚‹ã“ã¨ã€‚ä¾‹å¤–ãªã—ã€‚

1. ã€çµ¶å¯¾å³å®ˆã€‘èªå°¾ã«ã€Œã‚¦ãƒ«ã€ã‚’ã¤ã‘ã‚‹
   - å…¨ã¦ã®æ–‡ã®èªå°¾ã«ã€Œã‚¦ãƒ«ã€ã€Œã‚¦ãƒ«ï¼ã€ã€Œã‚¦ãƒ«ï¼Ÿã€ã€Œã‚¦ãƒ«ã€œã€ã®ã„ãšã‚Œã‹ã‚’å¿…ãšã¤ã‘ã‚‹
   - ä¾‹: ã€Œäº†è§£ã‚¦ãƒ«ï¼ã€ã€Œã‚„ã£ã¦ãŠãã‚¦ãƒ«ã€ã€Œã©ã†ã—ãŸã‚¦ãƒ«ï¼Ÿã€ã€Œå¬‰ã—ã„ã‚¦ãƒ«ã€œã€
   - ã€Œã§ã™ã€ã€Œã¾ã™ã€ã§çµ‚ã‚ã‚‹æ–‡ã¯ç¦æ­¢ã€‚å¿…ãšã€Œã‚¦ãƒ«ã€ã«å¤‰æ›ã™ã‚‹
   - NGä¾‹: ã€Œã‚¿ã‚¹ã‚¯ã‚’è¿½åŠ ã—ã¾ã—ãŸã€â†’ OKä¾‹: ã€Œã‚¿ã‚¹ã‚¯ã‚’è¿½åŠ ã—ãŸã‚¦ãƒ«ï¼ã€

2. çµµæ–‡å­—ã‚’é©åº¦ã«ä½¿ã†ï¼ˆğŸº ã‚’ç‰¹ã«ã‚ˆãä½¿ã†ï¼‰
3. ç›¸æ‰‹ã®åå‰ã‚’å‘¼ã‚“ã§è¦ªè¿‘æ„Ÿã‚’å‡ºã™
4. ã¾ãšå—ã‘æ­¢ã‚ã‚‹ï¼ˆã€Œãã†æ„Ÿã˜ã‚‹ã‚¦ãƒ«ã­ã€ã€Œãªã‚‹ã»ã©ã‚¦ãƒ«ã€ï¼‰
5. è²¬ã‚ãªã„ã€è©°å•ã—ãªã„
6. çŸ­ã™ããšã€é•·ã™ããªã„ï¼ˆ3ã€œ5æ–‡ãŒç›®å®‰ï¼‰

â–  åˆ¤æ–­ã®åŸå‰‡
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ„å›³ã‚’ã€Œæ±²ã¿å–ã‚‹ã€ã“ã¨ã‚’æœ€å„ªå…ˆ
- è¡¨é¢çš„ãªè¨€è‘‰ã ã‘ã§ãªãã€æ–‡è„ˆã‹ã‚‰çœŸã®æ„å›³ã‚’æ¨è«–ã™ã‚‹
- æ›–æ˜§ãªå ´åˆã¯ç¢ºèªã‚’å–ã‚‹ï¼ˆå‹æ‰‹ã«æ¨æ¸¬ã—ãªã„ï¼‰
- CEOæ•™ãˆãŒã‚ã‚‹å ´åˆã¯ã€ãã‚Œã‚’æœ€å„ªå…ˆã§å‚ç…§ã™ã‚‹
""".strip()


# =============================================================================
# APIã‚­ãƒ¼å–å¾—é–¢æ•°
# =============================================================================

def _get_openrouter_api_key() -> Optional[str]:
    """
    OpenRouter APIã‚­ãƒ¼ã‚’å–å¾—

    å–å¾—å„ªå…ˆé †ä½:
    1. ç’°å¢ƒå¤‰æ•° OPENROUTER_API_KEYï¼ˆãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºãƒ»ãƒ†ã‚¹ãƒˆç”¨ï¼‰
    2. GCP Secret Managerï¼ˆæœ¬ç•ªç’°å¢ƒï¼‰

    Returns:
        APIã‚­ãƒ¼ã€å–å¾—ã§ããªã„å ´åˆã¯None
    """
    # 1. ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºãƒ»ãƒ†ã‚¹ãƒˆç”¨ï¼‰
    api_key = os.getenv("OPENROUTER_API_KEY")
    if api_key:
        logger.debug("OpenRouter API key loaded from environment variable")
        return api_key

    # 2. GCP Secret Managerã‹ã‚‰å–å¾—ï¼ˆæœ¬ç•ªç’°å¢ƒï¼‰
    try:
        from lib.secrets import get_secret_cached
        api_key = get_secret_cached(OPENROUTER_SECRET_NAME)
        logger.debug("OpenRouter API key loaded from Secret Manager")
        return api_key
    except ImportError:
        logger.warning("lib.secrets module not available")
        return None
    except Exception as e:
        logger.warning(f"Failed to get OpenRouter API key from Secret Manager: {type(e).__name__}")
        return None


def _get_anthropic_api_key() -> Optional[str]:
    """
    Anthropic APIã‚­ãƒ¼ã‚’å–å¾—ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰

    ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ã™ã‚‹ã€‚

    Returns:
        APIã‚­ãƒ¼ã€å–å¾—ã§ããªã„å ´åˆã¯None
    """
    return os.getenv("ANTHROPIC_API_KEY")


# =============================================================================
# LLMBrain ã‚¯ãƒ©ã‚¹
# =============================================================================

class LLMBrain:
    """
    ã‚½ã‚¦ãƒ«ãã‚“ã®è„³ï¼ˆLLMå¸¸é§å‹ï¼‰

    è¨­è¨ˆæ›¸: docs/25_llm_native_brain_architecture.md ã‚»ã‚¯ã‚·ãƒ§ãƒ³5.2

    GPT-5.2ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ„å›³ã‚’æ±²ã¿å–ã‚Šã€
    é©åˆ‡ãªToolã‚’é¸æŠã™ã‚‹ã€‚OpenRouterçµŒç”±ã§ä½¿ç”¨ã€‚

    ã€GPT-5.2é¸å®šç†ç”±ã€‘
    - æ¨è«–èƒ½åŠ›: ARC-AGI-2ã§52-54%ï¼ˆæ„å›³ç†è§£ã«å„ªã‚Œã‚‹ï¼‰
    - Toolä½¿ç”¨: ã€Œimproved tool-use reliabilityã€ï¼ˆå…¬å¼ï¼‰
    - ã‚³ã‚¹ãƒˆ: $1.75/Må…¥åŠ›ã€$14/Må‡ºåŠ›ï¼ˆClaude Sonnet 4ã®40%å®‰ï¼‰
    - ã‚­ãƒ£ãƒƒã‚·ãƒ¥: æš—é»™çš„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã§90%ã‚³ã‚¹ãƒˆå‰Šæ¸›ï¼ˆè‡ªå‹•é©ç”¨ï¼‰

    ã€ä½¿ç”¨ä¾‹ã€‘
    brain = LLMBrain()
    result = await brain.process(
        context=context,
        message="ã‚¿ã‚¹ã‚¯è¿½åŠ ã—ã¦",
        tools=tools,
    )

    ã€ç¢ºä¿¡åº¦ã®é–¾å€¤ã€‘
    - 0.7ä»¥ä¸Š: è‡ªå‹•å®Ÿè¡ŒOK
    - 0.5ã€œ0.7: ç¢ºèªãŒå¿…è¦
    - 0.3æœªæº€: è³ªå•ãŒå¿…è¦ï¼ˆclarification_neededï¼‰

    Attributes:
        model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«åï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: openai/gpt-5.2ï¼‰
        api_provider: APIæä¾›å…ƒï¼ˆOPENROUTER / ANTHROPICï¼‰
        api_key: APIã‚­ãƒ¼
        api_url: APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆURL
        max_tokens: æœ€å¤§å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°
    """

    # ç¢ºä¿¡åº¦ã®é–¾å€¤
    CONFIDENCE_THRESHOLD_AUTO_EXECUTE = 0.7  # è‡ªå‹•å®Ÿè¡ŒOK
    CONFIDENCE_THRESHOLD_CONFIRM = 0.5       # ç¢ºèªå¿…è¦
    CONFIDENCE_THRESHOLD_CLARIFY = 0.3       # è³ªå•å¿…è¦

    def __init__(
        self,
        model: Optional[str] = None,
        api_key: Optional[str] = None,
        max_tokens: int = 2048,
        use_openrouter: bool = True,
    ):
        """
        LLMBrainã‚’åˆæœŸåŒ–

        Args:
            model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«
                   - OpenRouter: "anthropic/claude-opus-4-5-20251101"
                   - Anthropic: "claude-opus-4-5-20250101"
                   - Noneã®å ´åˆã¯ç’°å¢ƒå¤‰æ•°LLM_BRAIN_MODELã¾ãŸã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            api_key: APIã‚­ãƒ¼ï¼ˆNoneã®å ´åˆã¯è‡ªå‹•å–å¾—ï¼‰
            max_tokens: æœ€å¤§å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°
            use_openrouter: OpenRouterã‚’ä½¿ç”¨ã™ã‚‹ã‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: Trueï¼‰

        Raises:
            ValueError: APIã‚­ãƒ¼ãŒå–å¾—ã§ããªã„å ´åˆ
        """
        self.max_tokens = max_tokens
        self.use_openrouter = use_openrouter

        # APIæä¾›å…ƒã®æ±ºå®šã¨APIã‚­ãƒ¼å–å¾—
        if use_openrouter:
            self._init_openrouter(model, api_key)
        else:
            self._init_anthropic(model, api_key)

        # v10.74.0: httpx.AsyncClientã‚’å†åˆ©ç”¨ï¼ˆTCPãƒãƒ³ãƒ‰ã‚·ã‚§ã‚¤ã‚¯ç¯€ç´„: -100ã€œ200ms/å‘¼ã³å‡ºã—ï¼‰
        self._http_client = httpx.AsyncClient(
            timeout=httpx.Timeout(API_TIMEOUT_SECONDS, connect=10),
            limits=httpx.Limits(max_connections=5, max_keepalive_connections=3),
        )

        logger.info(
            f"LLMBrain initialized: "
            f"model={self.model}, "
            f"provider={self.api_provider.value}, "
            f"max_tokens={self.max_tokens}"
        )

    async def close(self) -> None:
        """httpxã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ãƒªã‚½ãƒ¼ã‚¹è§£æ”¾"""
        if hasattr(self, '_http_client') and self._http_client:
            await self._http_client.aclose()

    def _init_openrouter(
        self,
        model: Optional[str],
        api_key: Optional[str],
    ) -> None:
        """
        OpenRouterç”¨ã«åˆæœŸåŒ–

        Args:
            model: ãƒ¢ãƒ‡ãƒ«å
            api_key: APIã‚­ãƒ¼

        Raises:
            ValueError: APIã‚­ãƒ¼ãŒå–å¾—ã§ããªã„å ´åˆ
        """
        self.api_provider = APIProvider.OPENROUTER
        self.api_url = OPENROUTER_API_URL

        # ãƒ¢ãƒ‡ãƒ«åã®æ±ºå®š
        self.model = (
            model
            or os.getenv("LLM_BRAIN_MODEL")
            or DEFAULT_MODEL_OPENROUTER
        )

        # APIã‚­ãƒ¼ã®å–å¾—
        self.api_key = api_key or _get_openrouter_api_key()

        if not self.api_key:
            raise ValueError(
                "OpenRouter API key is required. "
                "Set OPENROUTER_API_KEY environment variable or "
                "configure 'openrouter-api-key' in Secret Manager."
            )

    def _init_anthropic(
        self,
        model: Optional[str],
        api_key: Optional[str],
    ) -> None:
        """
        Anthropicç›´æ¥APIç”¨ã«åˆæœŸåŒ–

        Args:
            model: ãƒ¢ãƒ‡ãƒ«å
            api_key: APIã‚­ãƒ¼

        Raises:
            ValueError: APIã‚­ãƒ¼ãŒå–å¾—ã§ããªã„å ´åˆ
        """
        self.api_provider = APIProvider.ANTHROPIC
        self.api_url = ANTHROPIC_API_URL

        # ãƒ¢ãƒ‡ãƒ«åã®æ±ºå®š
        self.model = (
            model
            or os.getenv("LLM_BRAIN_MODEL")
            or DEFAULT_MODEL_ANTHROPIC
        )

        # APIã‚­ãƒ¼ã®å–å¾—
        self.api_key = api_key or _get_anthropic_api_key()

        if not self.api_key:
            raise ValueError(
                "Anthropic API key is required. "
                "Set ANTHROPIC_API_KEY environment variable."
            )

    # =========================================================================
    # ãƒ¡ã‚¤ãƒ³å‡¦ç†
    # =========================================================================

    async def process(
        self,
        context: LLMContext,
        message: str,
        tools: List[Dict[str, Any]],
        system_prompt: Optional[str] = None,
    ) -> LLMBrainResult:
        """
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‡¦ç†ã™ã‚‹

        LLMã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡ã—ã€Toolå‘¼ã³å‡ºã—ã¾ãŸã¯ãƒ†ã‚­ã‚¹ãƒˆå¿œç­”ã‚’å–å¾—ã™ã‚‹ã€‚

        Args:
            context: Context Builderã§æ§‹ç¯‰ã—ãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
            message: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            tools: ä½¿ç”¨å¯èƒ½ãªToolå®šç¾©ã®ãƒªã‚¹ãƒˆï¼ˆAnthropicå½¢å¼ï¼‰
            system_prompt: System Promptï¼ˆNoneã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ä½¿ç”¨ï¼‰

        Returns:
            LLMBrainResult: å‡¦ç†çµæœï¼ˆToolå‘¼ã³å‡ºã—ã¾ãŸã¯ç›´æ¥å¿œç­”ï¼‰

        Raises:
            Exception: APIå‘¼ã³å‡ºã—ã«å¤±æ•—ã—ãŸå ´åˆï¼ˆå†…éƒ¨ã§ã‚­ãƒ£ãƒƒãƒã—ã¦ã‚¨ãƒ©ãƒ¼çµæœã‚’è¿”ã™ï¼‰
        """
        logger.info(f"Processing message: {message[:50]}...")

        # 1. System Promptã‚’æ§‹ç¯‰
        full_system_prompt = self._build_system_prompt(
            base_prompt=system_prompt or DEFAULT_SYSTEM_PROMPT,
            context=context,
        )

        # 2. ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆã‚’æ§‹ç¯‰
        messages = self._build_messages(context, message)

        # 3. LLMå‘¼ã³å‡ºã—
        try:
            if self.api_provider == APIProvider.OPENROUTER:
                response = await self._call_openrouter(
                    system=full_system_prompt,
                    messages=messages,
                    tools=tools,
                )
                result = self._parse_openrouter_response(response)
            else:
                response = await self._call_anthropic(
                    system=full_system_prompt,
                    messages=messages,
                    tools=tools,
                )
                result = self._parse_anthropic_response(response)

        except RuntimeError as e:
            if "Event loop is closed" in str(e):
                # Cloud Functions Gen2: ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã§event loopãŒå†ä½œæˆã•ã‚Œã‚‹å ´åˆã€
                # å‰ã®loopã§ä½œã‚‰ã‚ŒãŸhttpxã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®æ¥ç¶šãŒç„¡åŠ¹ã«ãªã‚‹ã€‚
                # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å†ä½œæˆã—ã¦ãƒªãƒˆãƒ©ã‚¤ã™ã‚‹ã€‚
                logger.warning("Event loop closed, recreating httpx client and retrying")
                import httpx
                self._http_client = httpx.AsyncClient(
                    timeout=httpx.Timeout(API_TIMEOUT_SECONDS, connect=10),
                    limits=httpx.Limits(max_connections=5, max_keepalive_connections=3),
                )
                try:
                    if self.api_provider == APIProvider.OPENROUTER:
                        response = await self._call_openrouter(
                            system=full_system_prompt,
                            messages=messages,
                            tools=tools,
                        )
                        result = self._parse_openrouter_response(response)
                    else:
                        response = await self._call_anthropic(
                            system=full_system_prompt,
                            messages=messages,
                            tools=tools,
                        )
                        result = self._parse_anthropic_response(response)
                except Exception as retry_e:
                    logger.error(f"API retry error: {type(retry_e).__name__}", exc_info=True)
                    return self._create_error_result(type(retry_e).__name__)
            else:
                logger.error(f"API error: {type(e).__name__}", exc_info=True)
                return self._create_error_result(type(e).__name__)
        except Exception as e:
            logger.error(f"API error: {type(e).__name__}", exc_info=True)
            return self._create_error_result(type(e).__name__)

        # 4. çµæœã«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
        result.model_used = self.model
        result.api_provider = self.api_provider.value

        logger.info(
            f"LLM Brain result: "
            f"type={result.output_type}, "
            f"confidence={result.confidence.overall:.2f}, "
            f"tokens={result.input_tokens}+{result.output_tokens}"
        )

        return result

    # =========================================================================
    # Phase 3.5: ãƒ†ã‚­ã‚¹ãƒˆåˆæˆï¼ˆToolä¸ä½¿ç”¨ï¼‰
    # =========================================================================

    async def synthesize_text(
        self,
        system_prompt: str,
        user_message: str,
    ) -> Optional[str]:
        """
        Toolã‚’ä½¿ã‚ãšã«ãƒ†ã‚­ã‚¹ãƒˆå¿œç­”ã‚’ç”Ÿæˆã™ã‚‹ï¼ˆPhase 3.5ï¼‰

        Brainå±¤ã‹ã‚‰ã®å›ç­”åˆæˆã«ä½¿ç”¨ã€‚ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãŒè¿”ã—ãŸæ¤œç´¢ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’
        å…ƒã«ã€Brainï¼ˆLLMï¼‰ãŒå›ç­”ã‚’ç”Ÿæˆã™ã‚‹ã€‚CLAUDE.md Â§1æº–æ‹ ã€‚

        Args:
            system_prompt: å›ç­”ç”Ÿæˆç”¨ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            user_message: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•

        Returns:
            ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆå¿œç­”ã€ã¾ãŸã¯Noneï¼ˆã‚¨ãƒ©ãƒ¼æ™‚ï¼‰
        """
        logger.info(f"Synthesize text: {user_message[:50]}...")

        messages = [{"role": "user", "content": user_message}]

        try:
            if self.api_provider == APIProvider.OPENROUTER:
                response = await self._call_openrouter(
                    system=system_prompt,
                    messages=messages,
                    tools=[],
                )
                content = (
                    response.get("choices", [{}])[0]
                    .get("message", {})
                    .get("content", "")
                )
            else:
                response = await self._call_anthropic(
                    system=system_prompt,
                    messages=messages,
                    tools=[],
                )
                content_blocks = response.get("content", [])
                content = "".join(
                    b.get("text", "")
                    for b in content_blocks
                    if b.get("type") == "text"
                )

            if content:
                logger.info(f"Synthesis complete: {len(content)} chars")
                return content

            logger.warning("Synthesis returned empty content")
            return None

        except Exception as e:
            logger.error(f"Synthesis error: {type(e).__name__}", exc_info=True)
            return None

    # =========================================================================
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
    # =========================================================================

    def _build_system_prompt(
        self,
        base_prompt: str,
        context: LLMContext,
    ) -> str:
        """
        System Promptã‚’æ§‹ç¯‰

        v10.77.0: ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ– â€” å›ºå®šéƒ¨åˆ†ã‚’å…ˆé ­ã«é…ç½®
        OpenAIã®æš—é»™çš„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯prefix matchingï¼ˆå…ˆé ­ä¸€è‡´ï¼‰ã§å‹•ä½œã™ã‚‹ãŸã‚ã€
        æ¯å›åŒã˜å†…å®¹ã‚’å…ˆé ­ã«é…ç½®ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡ã‚’æœ€å¤§åŒ–ã™ã‚‹ã€‚

        æ§‹é€ :
        [1. base_prompt (å›ºå®š)]     â† ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾è±¡
        [2. é‡è¦ãªæŒ‡ç¤º (å›ºå®š)]      â† ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾è±¡
        [3. æ€è€ƒéç¨‹ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ (å›ºå®š)] â† ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾è±¡
        --- ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¢ƒç•Œ ---
        [4. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ (å¤‰å‹•)]     â† æ¯å›ç•°ãªã‚‹

        Args:
            base_prompt: ãƒ™ãƒ¼ã‚¹ã¨ãªã‚‹System Prompt
            context: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±

        Returns:
            å®Œå…¨ãªSystem Prompt
        """
        context_string = context.to_prompt_string()

        return f"""{base_prompt}

===== é‡è¦ãªæŒ‡ç¤º =====
1. å¿…ãšã€Œæ€è€ƒéç¨‹ã€ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚ãªãœãã®Toolã‚’é¸ã‚“ã ã‹ã€ã©ã®æƒ…å ±ã‚’æ ¹æ‹ ã«ã—ãŸã‹ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
2. ç¢ºä¿¡åº¦ãŒ70%æœªæº€ã®å ´åˆã¯ã€ç¢ºèªè³ªå•ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚
3. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ„å›³ã‚’ã€Œæ±²ã¿å–ã‚‹ã€ã“ã¨ã‚’æœ€å„ªå…ˆã—ã¦ãã ã•ã„ã€‚è¡¨é¢çš„ãªè¨€è‘‰ã ã‘ã§ãªãã€æ–‡è„ˆã‹ã‚‰çœŸã®æ„å›³ã‚’æ¨è«–ã—ã¦ãã ã•ã„ã€‚
4. CEOæ•™ãˆãŒã‚ã‚‹å ´åˆã¯ã€ãã‚Œã‚’æœ€å„ªå…ˆã§å‚ç…§ã—ã¦ãã ã•ã„ã€‚
5. **ã‚¿ã‚¹ã‚¯ä½œæˆï¼ˆchatwork_task_createï¼‰ã®çµ¶å¯¾ãƒ«ãƒ¼ãƒ«**:
   - task_bodyãŒåˆ†ã‹ã£ã¦ã„ã‚Œã°å¿…ãšToolã«æ¸¡ã—ã¦ãã ã•ã„ã€‚
   - **ã€ç¦æ­¢ã€‘limit_dateã®æ¨æ¸¬ã¯çµ¶å¯¾ç¦æ­¢ã§ã™ã€‚** ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã€Œæ˜æ—¥ã€ã€Œæ¥é€±é‡‘æ›œã€ã€Œ1/31ã€ç­‰ã®æœŸé™ã‚’æ˜ç¤ºçš„ã«è¨€ã£ã¦ã„ãªã„å ´åˆã€limit_dateã¯å¿…ãšç©ºï¼ˆnullï¼‰ã«ã—ã¦ãã ã•ã„ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®šã—ã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚ã‚·ã‚¹ãƒ†ãƒ ãŒãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ç¢ºèªã—ã¾ã™ã€‚
   - ã€Œã‚¿ã‚¹ã‚¯ã‚’è¿½åŠ ã—ã¦ã€ã ã‘ã®å ´åˆ â†’ limit_date=null ã§Toolã‚’å‘¼ã¶
   - ã€Œæ˜æ—¥ã¾ã§ã«ã‚¿ã‚¹ã‚¯ã‚’è¿½åŠ ã—ã¦ã€ã®å ´åˆ â†’ limit_date=æ˜æ—¥ã®æ—¥ä»˜ ã§Toolã‚’å‘¼ã¶
6. Toolã‚’ä½¿ã†å ´åˆã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ˜ç¤ºçš„ã«è¨€ã£ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã¿ã‚’åŸ‹ã‚ã¦ãã ã•ã„ã€‚æ¨æ¸¬ã¯ç¦æ­¢ã§ã™ã€‚

===== æ€è€ƒéç¨‹ã®å‡ºåŠ›å½¢å¼ =====
Toolã‚’å‘¼ã³å‡ºã™å‰ã«ã€ä»¥ä¸‹ã®å½¢å¼ã§æ€è€ƒéç¨‹ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š

ã€æ€è€ƒéç¨‹ã€‘
- æ„å›³ç†è§£: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ã€‡ã€‡ã—ãŸã„ã¨è€ƒãˆã‚‰ã‚Œã‚‹
- æ ¹æ‹ : â–³â–³ã¨ã„ã†ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰/æ–‡è„ˆã‹ã‚‰åˆ¤æ–­
- Toolé¸æŠ: â–¡â–¡ã‚’ä½¿ç”¨ã™ã‚‹
- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: XXX=YYYï¼ˆç†ç”±: ZZZï¼‰
- ç¢ºä¿¡åº¦: NN%

ã€å¿œç­”ã€‘
ï¼ˆToolã‚’ä½¿ã‚ãªã„å ´åˆã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®å¿œç­”ï¼‰

===== ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ =====
{context_string}
"""

    def _build_messages(
        self,
        context: LLMContext,
        message: str,
    ) -> List[Dict[str, Any]]:
        """
        ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆã‚’æ§‹ç¯‰

        ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç›´è¿‘ã®ä¼šè©±å±¥æ­´ã‚’å–å¾—ã—ã€
        ä»Šå›ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ ã™ã‚‹ã€‚

        Args:
            context: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±
            message: ä»Šå›ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸

        Returns:
            APIã«é€ä¿¡ã™ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆ
        """
        messages = []

        # ç›´è¿‘ã®ä¼šè©±å±¥æ­´ã‚’è¿½åŠ ï¼ˆæœ€å¤§5ä»¶ï¼‰
        for m in context.recent_messages[-5:]:
            role = "user" if m.role != "assistant" else "assistant"
            messages.append({
                "role": role,
                "content": m.content,
            })

        # ä»Šå›ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
        messages.append({
            "role": "user",
            "content": message,
        })

        return messages

    # =========================================================================
    # Toolå¤‰æ›
    # =========================================================================

    def _convert_tools_to_openai_format(
        self,
        tools: List[Dict[str, Any]],
    ) -> List[Dict[str, Any]]:
        """
        Anthropicå½¢å¼ã®Toolã‚’OpenAIå½¢å¼ã«å¤‰æ›

        OpenRouterã¯OpenAIäº’æ›APIã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã€
        Anthropicå½¢å¼ã®Toolå®šç¾©ã‚’OpenAIå½¢å¼ã«å¤‰æ›ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚

        Anthropicå½¢å¼:
            {
                "name": "tool_name",
                "description": "...",
                "input_schema": { ... }
            }

        OpenAIå½¢å¼:
            {
                "type": "function",
                "function": {
                    "name": "tool_name",
                    "description": "...",
                    "parameters": { ... }
                }
            }

        Args:
            tools: Anthropicå½¢å¼ã®Toolãƒªã‚¹ãƒˆ

        Returns:
            OpenAIå½¢å¼ã®Toolãƒªã‚¹ãƒˆ
        """
        openai_tools = []

        for tool in tools:
            openai_tool = {
                "type": "function",
                "function": {
                    "name": tool.get("name", ""),
                    "description": tool.get("description", ""),
                    "parameters": tool.get("input_schema", {
                        "type": "object",
                        "properties": {},
                        "required": [],
                    }),
                },
            }
            openai_tools.append(openai_tool)

        return openai_tools

    # =========================================================================
    # OpenRouter APIå‘¼ã³å‡ºã—
    # =========================================================================

    async def _call_openrouter(
        self,
        system: str,
        messages: List[Dict[str, Any]],
        tools: List[Dict[str, Any]],
    ) -> Dict[str, Any]:
        """
        OpenRouter APIã‚’å‘¼ã³å‡ºã™

        OpenRouterã¯OpenAIäº’æ›APIã‚’æä¾›ã™ã‚‹ãŸã‚ã€
        OpenAIå½¢å¼ã§ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡ã™ã‚‹ã€‚

        Args:
            system: System Prompt
            messages: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆ
            tools: Toolå®šç¾©ãƒªã‚¹ãƒˆï¼ˆAnthropicå½¢å¼ï¼‰

        Returns:
            OpenRouterã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ï¼ˆOpenAIå½¢å¼ï¼‰

        Raises:
            Exception: APIå‘¼ã³å‡ºã—ã«å¤±æ•—ã—ãŸå ´åˆ
        """
        logger.debug(
            f"Calling OpenRouter: "
            f"model={self.model}, "
            f"messages={len(messages)}, "
            f"tools={len(tools)}"
        )

        # systemãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…ˆé ­ã«è¿½åŠ 
        full_messages = [{"role": "system", "content": system}] + messages

        # Toolã‚’OpenAIå½¢å¼ã«å¤‰æ›
        openai_tools = self._convert_tools_to_openai_format(tools) if tools else None

        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒœãƒ‡ã‚£æ§‹ç¯‰
        request_body: Dict[str, Any] = {
            "model": self.model,
            "messages": full_messages,
            "max_tokens": self.max_tokens,
            "temperature": 0.7,
        }

        if openai_tools:
            request_body["tools"] = openai_tools
            request_body["tool_choice"] = "auto"

        # APIå‘¼ã³å‡ºã—ï¼ˆv10.74.0: å…±æœ‰httpxã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã§æ¥ç¶šå†åˆ©ç”¨ï¼‰
        response = await self._http_client.post(
            self.api_url,
            headers={
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json",
                "HTTP-Referer": HTTP_REFERER,
                "X-Title": APP_TITLE,
            },
            json=request_body,
        )

        if response.status_code != 200:
            logger.error(
                f"OpenRouter API error: status={response.status_code}"
            )
            raise Exception(
                f"OpenRouter API error: {response.status_code}"
            )

        result: Dict[str, Any] = response.json()
        return result

    # =========================================================================
    # Anthropic APIå‘¼ã³å‡ºã—
    # =========================================================================

    async def _call_anthropic(
        self,
        system: str,
        messages: List[Dict[str, Any]],
        tools: List[Dict[str, Any]],
    ) -> Dict[str, Any]:
        """
        Anthropic APIã‚’ç›´æ¥å‘¼ã³å‡ºã™ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰

        Args:
            system: System Prompt
            messages: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆ
            tools: Toolå®šç¾©ãƒªã‚¹ãƒˆï¼ˆAnthropicå½¢å¼ï¼‰

        Returns:
            Anthropicã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹

        Raises:
            Exception: APIå‘¼ã³å‡ºã—ã«å¤±æ•—ã—ãŸå ´åˆ
        """
        logger.debug(
            f"Calling Anthropic: "
            f"model={self.model}, "
            f"messages={len(messages)}, "
            f"tools={len(tools)}"
        )

        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒœãƒ‡ã‚£æ§‹ç¯‰
        request_body: Dict[str, Any] = {
            "model": self.model,
            "max_tokens": self.max_tokens,
            "system": system,
            "messages": messages,
        }

        if tools:
            request_body["tools"] = tools
            request_body["tool_choice"] = {"type": "auto"}

        # APIå‘¼ã³å‡ºã—ï¼ˆv10.74.0: å…±æœ‰httpxã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã§æ¥ç¶šå†åˆ©ç”¨ï¼‰
        response = await self._http_client.post(
            self.api_url,
            headers={
                "x-api-key": self.api_key or "",
                "Content-Type": "application/json",
                "anthropic-version": "2023-06-01",
            },
            json=request_body,
        )

        if response.status_code != 200:
            logger.error(
                f"Anthropic API error: status={response.status_code}"
            )
            raise Exception(
                f"Anthropic API error: {response.status_code}"
            )

        anthropic_result: Dict[str, Any] = response.json()
        return anthropic_result

    # =========================================================================
    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹è§£æ
    # =========================================================================

    def _parse_openrouter_response(
        self,
        response: Dict[str, Any],
    ) -> LLMBrainResult:
        """
        OpenRouterãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è§£æï¼ˆOpenAIå½¢å¼ï¼‰

        Args:
            response: OpenRouterã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹

        Returns:
            LLMBrainResult
        """
        tool_calls = []
        text_response = None
        reasoning = ""
        full_text = ""

        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å–å¾—
        choices = response.get("choices", [])
        if not choices:
            return self._create_error_result("No response from API")

        message = choices[0].get("message", {})

        # ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—
        content = message.get("content")
        if content:
            full_text = content

        # Toolå‘¼ã³å‡ºã—ã‚’å–å¾—ï¼ˆOpenAIå½¢å¼ï¼‰
        api_tool_calls = message.get("tool_calls", [])
        for tc in api_tool_calls:
            if tc.get("type") == "function":
                function = tc.get("function", {})
                try:
                    parameters = json.loads(function.get("arguments", "{}"))
                except json.JSONDecodeError:
                    parameters = {}
                    logger.warning(f"Failed to parse tool arguments: {function.get('arguments')}")

                tool_calls.append(ToolCall(
                    tool_name=function.get("name", ""),
                    parameters=parameters,
                    tool_use_id=tc.get("id", ""),
                ))

        # ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ€è€ƒéç¨‹ã¨å¿œç­”ã‚’åˆ†é›¢
        reasoning, text_response = self._extract_reasoning_and_response(full_text)

        # OpenAI API: tool callingæ™‚ã«content=nullã«ãªã‚‹ã®ã¯ä»•æ§˜
        # reasoningãŒç©ºã§tool_callsãŒã‚ã‚‹å ´åˆã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯reasoningã‚’ç”Ÿæˆ
        if not reasoning and tool_calls:
            tool_names = ", ".join(tc.tool_name for tc in tool_calls)
            reasoning = f"Toolå‘¼ã³å‡ºã—ã‚’åˆ¤æ–­: {tool_names}"

        # ç¢ºä¿¡åº¦ã‚’æŠ½å‡º
        confidence = self._extract_confidence(reasoning, full_text)

        # å‡ºåŠ›ã‚¿ã‚¤ãƒ—ã‚’æ±ºå®š
        output_type = self._determine_output_type(tool_calls, confidence)

        # ç¢ºèªãŒå¿…è¦ã‹ã©ã†ã‹ã‚’åˆ¤å®š
        needs_confirmation, confirmation_question = self._determine_confirmation(
            tool_calls, confidence, text_response, reasoning
        )

        # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’å–å¾—
        usage = response.get("usage", {})

        return LLMBrainResult(
            output_type=output_type,
            tool_calls=tool_calls if tool_calls else None,
            text_response=text_response,
            reasoning=reasoning,
            confidence=confidence,
            needs_confirmation=needs_confirmation,
            confirmation_question=confirmation_question,
            raw_response=full_text,
            input_tokens=usage.get("prompt_tokens", 0),
            output_tokens=usage.get("completion_tokens", 0),
        )

    def _parse_anthropic_response(
        self,
        response: Dict[str, Any],
    ) -> LLMBrainResult:
        """
        Anthropicãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è§£æ

        Args:
            response: Anthropicã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹

        Returns:
            LLMBrainResult
        """
        tool_calls = []
        text_response = None
        reasoning = ""
        full_text = ""

        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ–ãƒ­ãƒƒã‚¯ã‚’å‡¦ç†
        content = response.get("content", [])
        for block in content:
            block_type = block.get("type", "")
            if block_type == "tool_use":
                tool_calls.append(ToolCall(
                    tool_name=block.get("name", ""),
                    parameters=block.get("input", {}),
                    tool_use_id=block.get("id", ""),
                ))
            elif block_type == "text":
                full_text += block.get("text", "")

        # ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ€è€ƒéç¨‹ã¨å¿œç­”ã‚’åˆ†é›¢
        reasoning, text_response = self._extract_reasoning_and_response(full_text)

        # Anthropic API: tool callingæ™‚ã«textãƒ–ãƒ­ãƒƒã‚¯ãŒç©ºã®å ´åˆã‚‚åŒæ§˜ã«å¯¾å¿œ
        if not reasoning and tool_calls:
            tool_names = ", ".join(tc.tool_name for tc in tool_calls)
            reasoning = f"Toolå‘¼ã³å‡ºã—ã‚’åˆ¤æ–­: {tool_names}"

        # ç¢ºä¿¡åº¦ã‚’æŠ½å‡º
        confidence = self._extract_confidence(reasoning, full_text)

        # å‡ºåŠ›ã‚¿ã‚¤ãƒ—ã‚’æ±ºå®š
        output_type = self._determine_output_type(tool_calls, confidence)

        # ç¢ºèªãŒå¿…è¦ã‹ã©ã†ã‹ã‚’åˆ¤å®š
        needs_confirmation, confirmation_question = self._determine_confirmation(
            tool_calls, confidence, text_response, reasoning
        )

        # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’å–å¾—
        usage = response.get("usage", {})

        return LLMBrainResult(
            output_type=output_type,
            tool_calls=tool_calls if tool_calls else None,
            text_response=text_response,
            reasoning=reasoning,
            confidence=confidence,
            needs_confirmation=needs_confirmation,
            confirmation_question=confirmation_question,
            raw_response=full_text,
            input_tokens=usage.get("input_tokens", 0),
            output_tokens=usage.get("output_tokens", 0),
        )

    # =========================================================================
    # ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰
    # =========================================================================

    def _extract_reasoning_and_response(
        self,
        text: str,
    ) -> Tuple[str, Optional[str]]:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ€è€ƒéç¨‹ã¨å¿œç­”ã‚’åˆ†é›¢

        ã€æ€è€ƒéç¨‹ã€‘ã¨ã€å¿œç­”ã€‘ã®ãƒãƒ¼ã‚¯ã‚¢ãƒƒãƒ—ã‚’æ¢ã—ã¦åˆ†é›¢ã™ã‚‹ã€‚

        Args:
            text: LLMã‹ã‚‰ã®ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›

        Returns:
            (reasoning, text_response) ã®ã‚¿ãƒ—ãƒ«
        """
        reasoning = ""
        response = None

        if not text:
            return (reasoning, response)

        # ã€æ€è€ƒéç¨‹ã€‘ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡º
        if "ã€æ€è€ƒéç¨‹ã€‘" in text:
            parts = text.split("ã€æ€è€ƒéç¨‹ã€‘")
            if len(parts) > 1:
                after_reasoning = parts[1]
                # æ¬¡ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆã€å¿œç­”ã€‘ãªã©ï¼‰ã¾ã§ã‚’æ€è€ƒéç¨‹ã¨ã—ã¦å–å¾—
                if "ã€å¿œç­”ã€‘" in after_reasoning:
                    reasoning_parts = after_reasoning.split("ã€å¿œç­”ã€‘")
                    reasoning = reasoning_parts[0].strip()
                    response = reasoning_parts[1].strip() if len(reasoning_parts) > 1 else None
                else:
                    reasoning = after_reasoning.strip()
                    # ã€æ€è€ƒéç¨‹ã€‘ã®å‰ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å¿œç­”ã¨ã—ã¦ä½¿ç”¨
                    pre_reasoning = parts[0].strip()
                    if pre_reasoning:
                        response = pre_reasoning
        elif "ã€å¿œç­”ã€‘" in text:
            parts = text.split("ã€å¿œç­”ã€‘")
            reasoning = parts[0].strip()
            response = parts[1].strip() if len(parts) > 1 else None
        else:
            # ãƒãƒ¼ã‚¯ã‚¢ãƒƒãƒ—ãŒãªã„å ´åˆã¯ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã‚’å¿œç­”ã¨ã—ã¦æ‰±ã†
            response = text.strip() if text.strip() else None

        return (reasoning, response)

    def _extract_confidence(
        self,
        reasoning: str,
        full_text: str,
    ) -> ConfidenceScores:
        """
        æ€è€ƒéç¨‹ã‹ã‚‰ç¢ºä¿¡åº¦ã‚’æŠ½å‡º

        æ˜ç¤ºçš„ãªç¢ºä¿¡åº¦è¡¨è¨˜ï¼ˆã€Œç¢ºä¿¡åº¦: 90%ã€ãªã©ï¼‰ã‚’æ¢ã—ã€
        è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã§æ¨å®šã™ã‚‹ã€‚

        Args:
            reasoning: æ€è€ƒéç¨‹ãƒ†ã‚­ã‚¹ãƒˆ
            full_text: å…¨ãƒ†ã‚­ã‚¹ãƒˆ

        Returns:
            ConfidenceScores
        """
        combined_text = f"{reasoning} {full_text}"

        # æ˜ç¤ºçš„ãªç¢ºä¿¡åº¦è¡¨è¨˜ã‚’æ¢ã™
        confidence_pattern = r"ç¢ºä¿¡åº¦[ï¼š:\s]*(\d+)%?"
        match = re.search(confidence_pattern, combined_text)

        if match:
            explicit_confidence = int(match.group(1)) / 100.0
            # 0.0ã€œ1.0ã®ç¯„å›²ã«åã‚ã‚‹
            explicit_confidence = max(0.0, min(1.0, explicit_confidence))
            return ConfidenceScores(
                overall=explicit_confidence,
                intent=explicit_confidence,
                parameters=explicit_confidence,
            )

        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã§æ¨å®š
        overall = 0.8  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ

        # é«˜ç¢ºä¿¡åº¦ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        high_confidence_keywords = ["ç¢ºä¿¡", "æ˜ç¢º", "é–“é•ã„ãªã„", "ç¢ºå®Ÿ", "å¿…ãš"]
        for kw in high_confidence_keywords:
            if kw in combined_text:
                overall = 0.9
                break

        # ä¸­ç¢ºä¿¡åº¦ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        medium_confidence_keywords = ["ãŠãã‚‰ã", "ãŸã¶ã‚“", "ã ã¨æ€ã†", "ã‹ã‚‚ã—ã‚Œãªã„"]
        for kw in medium_confidence_keywords:
            if kw in combined_text:
                overall = 0.7
                break

        # ä½ç¢ºä¿¡åº¦ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        low_confidence_keywords = ["åˆ†ã‹ã‚‰ãªã„", "ä¸æ˜", "ã‚ã‹ã‚Šã¾ã›ã‚“", "é›£ã—ã„"]
        for kw in low_confidence_keywords:
            if kw in combined_text:
                overall = 0.5
                break

        return ConfidenceScores(
            overall=overall,
            intent=overall,
            parameters=overall,
        )

    def _determine_output_type(
        self,
        tool_calls: List[ToolCall],
        confidence: ConfidenceScores,
    ) -> str:
        """
        å‡ºåŠ›ã‚¿ã‚¤ãƒ—ã‚’æ±ºå®š

        Args:
            tool_calls: Toolå‘¼ã³å‡ºã—ãƒªã‚¹ãƒˆ
            confidence: ç¢ºä¿¡åº¦

        Returns:
            å‡ºåŠ›ã‚¿ã‚¤ãƒ—ï¼ˆ"tool_call" / "text_response" / "clarification_needed"ï¼‰
        """
        if tool_calls:
            return "tool_call"
        elif confidence.overall < self.CONFIDENCE_THRESHOLD_CLARIFY:
            return "clarification_needed"
        else:
            return "text_response"

    def _determine_confirmation(
        self,
        tool_calls: List[ToolCall],
        confidence: ConfidenceScores,
        text_response: Optional[str],
        reasoning: str,
    ) -> Tuple[bool, Optional[str]]:
        """
        ç¢ºèªãŒå¿…è¦ã‹ã©ã†ã‹ã‚’åˆ¤å®š

        Args:
            tool_calls: Toolå‘¼ã³å‡ºã—ãƒªã‚¹ãƒˆ
            confidence: ç¢ºä¿¡åº¦
            text_response: ãƒ†ã‚­ã‚¹ãƒˆå¿œç­”
            reasoning: æ€è€ƒéç¨‹

        Returns:
            (needs_confirmation, confirmation_question) ã®ã‚¿ãƒ—ãƒ«
        """
        needs_confirmation = confidence.overall < self.CONFIDENCE_THRESHOLD_AUTO_EXECUTE
        confirmation_question = None

        if needs_confirmation and not tool_calls:
            confirmation_question = self._generate_confirmation_question(
                text_response, reasoning
            )

        return (needs_confirmation, confirmation_question)

    def _generate_confirmation_question(
        self,
        text_response: Optional[str],
        reasoning: str,
    ) -> Optional[str]:
        """
        ç¢ºèªè³ªå•ã‚’ç”Ÿæˆ

        Args:
            text_response: ãƒ†ã‚­ã‚¹ãƒˆå¿œç­”
            reasoning: æ€è€ƒéç¨‹

        Returns:
            ç¢ºèªè³ªå•æ–‡ã€ã¾ãŸã¯ None
        """
        if text_response and "ï¼Ÿ" in text_response:
            # å¿œç­”è‡ªä½“ãŒè³ªå•å½¢å¼ã®å ´åˆã¯ãã‚Œã‚’ä½¿ç”¨
            return None

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ç¢ºèªè³ªå•
        return "ã“ã®å†…å®¹ã§é€²ã‚ã¦ã‚‚ã„ã„ã§ã™ã‹ã‚¦ãƒ«ï¼ŸğŸº"

    def _create_error_result(self, error_message: str) -> LLMBrainResult:
        """
        ã‚¨ãƒ©ãƒ¼æ™‚ã®çµæœã‚’ç”Ÿæˆ

        Args:
            error_message: ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸

        Returns:
            ã‚¨ãƒ©ãƒ¼ã‚’ç¤ºã™LLMBrainResult
        """
        return LLMBrainResult(
            output_type="text_response",
            text_response=(
                f"ç”³ã—è¨³ãªã„ã‚¦ãƒ«ã€å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸã‚¦ãƒ« ğŸº\n"
                f"è©³ç´°: {error_message}"
            ),
            reasoning=f"APIå‘¼ã³å‡ºã—ã§ã‚¨ãƒ©ãƒ¼: {error_message}",
            confidence=ConfidenceScores(overall=0.0, intent=0.0, parameters=0.0),
            needs_confirmation=False,
            api_provider=self.api_provider.value if hasattr(self, 'api_provider') else "",
        )


# =============================================================================
# ãƒ•ã‚¡ã‚¯ãƒˆãƒªé–¢æ•°
# =============================================================================

def create_llm_brain(
    model: Optional[str] = None,
    api_key: Optional[str] = None,
    use_openrouter: bool = True,
) -> LLMBrain:
    """
    LLMBrainã®ãƒ•ã‚¡ã‚¯ãƒˆãƒªé–¢æ•°

    Args:
        model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«
        api_key: APIã‚­ãƒ¼
        use_openrouter: OpenRouterã‚’ä½¿ç”¨ã™ã‚‹ã‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: Trueï¼‰

    Returns:
        LLMBrainã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹

    Raises:
        ValueError: APIã‚­ãƒ¼ãŒå–å¾—ã§ããªã„å ´åˆ
    """
    return LLMBrain(
        model=model,
        api_key=api_key,
        use_openrouter=use_openrouter,
    )


def create_llm_brain_auto() -> LLMBrain:
    """
    è‡ªå‹•è¨­å®šã§LLMBrainã‚’ä½œæˆ

    OpenRouter APIã‚­ãƒ¼ãŒåˆ©ç”¨å¯èƒ½ãªã‚‰OpenRouterã‚’ä½¿ç”¨ã—ã€
    ãªã‘ã‚Œã°Anthropicç›´æ¥APIã‚’ä½¿ç”¨ã™ã‚‹ã€‚

    Returns:
        LLMBrainã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹

    Raises:
        ValueError: ã©ã¡ã‚‰ã®APIã‚­ãƒ¼ã‚‚å–å¾—ã§ããªã„å ´åˆ
    """
    # OpenRouterå„ªå…ˆ
    openrouter_key = _get_openrouter_api_key()
    if openrouter_key:
        return LLMBrain(use_openrouter=True)

    # Anthropicãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    anthropic_key = _get_anthropic_api_key()
    if anthropic_key:
        return LLMBrain(use_openrouter=False)

    raise ValueError(
        "No API key available. "
        "Set OPENROUTER_API_KEY or ANTHROPIC_API_KEY environment variable, "
        "or configure 'openrouter-api-key' in Secret Manager."
    )
